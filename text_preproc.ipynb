{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d6d5e69",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size: 28px;\"><b>Text Preprocessing</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcbf04e",
   "metadata": {},
   "source": [
    "**Goals of this Unit**\n",
    "\n",
    "The purpose of this unit is to learn how to prepare text data effectively for NLP tasks. After completing it, you will be able to:\n",
    "\n",
    "**Objectives**\n",
    "* Recognize several preprocessing techniques used in NLP.\n",
    "* Use **Python** and **regex** to clean and remove unnecessary formatting.\n",
    "* **Tokenize** text with **NLTK**.\n",
    "* **Normalize** text using Python, regex, and NLTK:\n",
    "\n",
    "  * remove affixes\n",
    "  * change text casing\n",
    "  * remove common words\n",
    "\n",
    "**Reminder**\n",
    "\n",
    "Learning is collaborative — engage with the Codecademy community through forums, share your progress, request code reviews, and review others' work to reinforce your understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc5e72c",
   "metadata": {},
   "source": [
    "# Natural Language Parsing with Regular Expressions\n",
    "\n",
    "Exploring hidden code words in declassified CIA files might sound like an intelligence officer’s job, and identifying gender bias in Harry Potter could seem like work for a literature expert. However, by applying natural language parsing with regular expressions, these kinds of analyses are accessible to anyone.\n",
    "\n",
    "Even if we don’t consciously think about sentence structure as we write, the way we build sentences is essential for conveying meaning. Studying how words are arranged — and which ones are chosen — can reveal deeper layers of a text. It can uncover connotations, expose an author’s biases, and provide insights that even close reading might miss.\n",
    "\n",
    "With Python’s re module for regular expressions and the Natural Language Toolkit (NLTK), you can search for key terms, analyze their frequency and context, and detect patterns in parts of speech to uncover subtle meanings within a text. Let’s dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39850b",
   "metadata": {},
   "source": [
    "## Compiling and Matching\n",
    "\n",
    "Regular expressions (regex) are used for **pattern matching in text**. In Python, the `re` module provides tools to work with regex.\n",
    "\n",
    "### Using `re.compile()`\n",
    "\n",
    "`re.compile()` creates a **regular expression object** based on a pattern:\n",
    "\n",
    "```python\n",
    "regular_expression_object = re.compile(\"[A-Za-z]{4}\")\n",
    "```\n",
    "\n",
    "This matches **exactly 4 alphabetic characters** (uppercase or lowercase).\n",
    "\n",
    "### Using `.match()`\n",
    "\n",
    "`.match()` checks if the pattern **starts at the beginning** of a string:\n",
    "\n",
    "```python\n",
    "result = regular_expression_object.match(\"Toto\")\n",
    "```\n",
    "\n",
    "* If there is a match → a **match object** is returned.\n",
    "* If not → `None` is returned.\n",
    "\n",
    "To retrieve the matched text, use:\n",
    "\n",
    "```python\n",
    "result.group(0)\n",
    "```\n",
    "\n",
    "### Shortcut — One Line\n",
    "\n",
    "You can skip compiling and match directly:\n",
    "\n",
    "```python\n",
    "result = re.match(\"[A-Za-z]{4}\", \"Toto\")\n",
    "```\n",
    "\n",
    "### Regex Meaning Table\n",
    "\n",
    "| Pattern    | Matches                             |\n",
    "| ---------- | ----------------------------------- |\n",
    "| `[A-Za-z]` | Any letter (uppercase or lowercase) |\n",
    "| `[A-Z]`    | Only uppercase letters              |\n",
    "| `[a-z]`    | Only lowercase letters              |\n",
    "| `\\d`       | Any digit (0–9)                     |\n",
    "| `.`        | Any character except newline        |\n",
    "\n",
    "These basics are essential before moving to more complex parsing tasks in NLP preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2067732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 7), match='Dorothy'>\n",
      "Dorothy\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# characters are defined\n",
    "character_1 = \"Dorothy\"\n",
    "character_2 = \"Henry\"\n",
    "\n",
    "# compile your regular expression here\n",
    "# [A-Za-z] means only letters (uppercase or lowercase), {7} means exactly 7 letters.\n",
    "regular_expression = re.compile(\"[A-Za-z]{7}\")\n",
    "\n",
    "# check for a match to character_1 here\n",
    "# .match() checks if the text starts with exactly 7 letters.\n",
    "result_1 = regular_expression.match(character_1)\n",
    "print(result_1)  # prints a match object if found, otherwise None\n",
    "\n",
    "# store and print the matched text here\n",
    "# .group(0) extracts the actual matched substring from the match object.\n",
    "match_1 = result_1.group(0)\n",
    "print(match_1)  # should print \"Dorothy\"\n",
    "\n",
    "# compile a regular expression to match a 7 character string of word characters and check for a match to character_2 here\n",
    "# NOTE: The \"!\" at the start makes the pattern invalid for matching — this prevents any match.\n",
    "result_2 = re.match(\"![A-Za-z]{7}\", character_2)\n",
    "print(result_2)  # will print None because of the leading \"!\" in the pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1694db1",
   "metadata": {},
   "source": [
    "## Searching and Finding\n",
    "\n",
    "The `.search()` method scans **an entire string** from left to right and returns the **first match** it finds. This differs from `.match()`, which only checks the **beginning** of a string.\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "result = re.search(\"\\w{8}\", \"Are you a Munchkin?\")\n",
    "```\n",
    "\n",
    "* `.search()` ➜ finds `\"Munchkin\"`\n",
    "* `.match()` ➜ returns `None` (because the match is not at the start)\n",
    "\n",
    "---\n",
    "\n",
    "### Finding All Matches — `.findall()`\n",
    "\n",
    "To retrieve **all non-overlapping matches**, use `.findall()`. It returns a **list** of every match found.\n",
    "\n",
    "Example text:\n",
    "\n",
    "```python\n",
    "text = \"Everything is green here, while in the country of the Munchkins blue was the favorite color...\"\n",
    "```\n",
    "\n",
    "To find all 8-character word sequences:\n",
    "\n",
    "```python\n",
    "list_of_matches = re.findall(\"\\w{8}\", text)\n",
    "```\n",
    "\n",
    "**Result:**\n",
    "\n",
    "```\n",
    "['Everythi', 'Munchkin', 'favorite', 'friendly', 'Munchkin']\n",
    "```\n",
    "\n",
    "`.findall()` is useful for **frequency analysis**, keyword extraction, and pattern detection in NLP preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe1d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "oz_text = open(\"utils/the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4437913c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(14, 20), match='wizard'>\n",
      "['lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion']\n",
      "183\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Search oz_text for the first occurrence of 'wizard'\n",
    "# .search() scans the entire text and returns a MATCH OBJECT if found, or None if not found.\n",
    "# It does NOT return True/False directly, but the match object can be used as a boolean.\n",
    "found_wizard = re.search(\"wizard\", oz_text)\n",
    "print(found_wizard)  # shows position of the first match (or None)\n",
    "\n",
    "# Step 2: Find all occurrences of 'lion' in the text\n",
    "# .findall() scans the entire text and returns a LIST of every match found.\n",
    "# Useful for frequency analysis and keyword extraction.\n",
    "all_lions = re.findall(\"lion\", oz_text)\n",
    "print(all_lions)  # prints every match of 'lion'\n",
    "\n",
    "# Step 3: Count the number of times 'lion' appears\n",
    "# len() gives the total frequency of matches in the text.\n",
    "number_lions = len(all_lions)\n",
    "print(number_lions)  # higher frequency may indicate higher importance in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0894d39d",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging\n",
    "\n",
    "To analyze text more meaningfully, we can examine it **word by word** and identify the **part of speech** (POS) of each word — a process known as **part-of-speech tagging**.\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "POS tagging helps us understand **how words function** within a sentence, which supports tasks like:\n",
    "\n",
    "* syntax analysis\n",
    "* sentiment analysis\n",
    "* information extraction\n",
    "* text generation\n",
    "\n",
    "---\n",
    "\n",
    "### Example Sentence\n",
    "\n",
    "**Wow! Ramona and her class are happily studying the new textbook she has on NLP.**\n",
    "\n",
    "| Part of Speech   | Examples                     |\n",
    "| ---------------- | ---------------------------- |\n",
    "| **Noun**         | Ramona, class, textbook, NLP |\n",
    "| **Pronoun**      | her, she                     |\n",
    "| **Determiner**   | the                          |\n",
    "| **Verb**         | studying, are, has           |\n",
    "| **Adjective**    | new                          |\n",
    "| **Adverb**       | happily                      |\n",
    "| **Preposition**  | on                           |\n",
    "| **Conjunction**  | and                          |\n",
    "| **Interjection** | Wow                          |\n",
    "\n",
    "---\n",
    "\n",
    "### POS Tagging with `nltk`\n",
    "\n",
    "Use `pos_tag()` to automatically tag words:\n",
    "\n",
    "```python\n",
    "word_sentence = ['do', 'you', 'suppose', 'oz', 'could', 'give', 'me', 'a', 'heart', '?']\n",
    "part_of_speech_tagged_sentence = pos_tag(word_sentence)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "[('do', 'VB'), ('you', 'PRP'), ('suppose', 'VB'), ('oz', 'NNS'),\n",
    " ('could', 'MD'), ('give', 'VB'), ('me', 'PRP'), ('a', 'DT'),\n",
    " ('heart', 'NN'), ('?', '.')]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Common POS Abbreviations\n",
    "\n",
    "| Tag | Meaning    |\n",
    "| --- | ---------- |\n",
    "| NN  | Noun       |\n",
    "| VB  | Verb       |\n",
    "| RB  | Adverb     |\n",
    "| JJ  | Adjective  |\n",
    "| DT  | Determiner |\n",
    "| PRP | Pronoun    |\n",
    "| MD  | Modal verb |\n",
    "\n",
    "You can find the full list of POS tags in the NLTK documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d28e3063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alamanna1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import the NLTK tokenizers for sentences and words\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Step 2: Make sure the Punkt tokenizer models are available (run once)\n",
    "# This is needed for both sentence and word tokenization.\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Step 3: Split the full text (oz_text) into individual sentences\n",
    "# sent_tokenize() takes a large string and returns a list of sentence strings.\n",
    "oz_sentences = sent_tokenize(oz_text)\n",
    "\n",
    "# Step 4: Tokenize each sentence into a list of word tokens\n",
    "# This will produce a list of lists, where each inner list contains tokens of one sentence.\n",
    "word_tokenized_oz = [word_tokenize(sentence) for sentence in oz_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a6e89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19c97414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokenized sentence at index 100: ['``', 'the', 'house', 'must', 'have', 'fallen', 'on', 'her', '.']\n",
      "POS-tagged version of the same sentence: [('``', '``'), ('the', 'DT'), ('house', 'NN'), ('must', 'MD'), ('have', 'VB'), ('fallen', 'VBN'), ('on', 'IN'), ('her', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Access and print the sentence at index 100\n",
    "witches_fate = word_tokenized_oz[100]\n",
    "print(\"Original tokenized sentence at index 100:\", witches_fate)\n",
    "\n",
    "# Step 2: Create an empty list to store POS-tagged sentences\n",
    "pos_tagged_oz = []\n",
    "\n",
    "# Step 3: Loop through all tokenized sentences and apply POS tagging\n",
    "for sentence in word_tokenized_oz:\n",
    "    tagged_sentence = pos_tag(sentence)  # assign part of speech to each token\n",
    "    pos_tagged_oz.append(tagged_sentence)\n",
    "\n",
    "# Step 4: Access and print the POS-tagged version of the sentence at index 100\n",
    "witches_fate_pos = pos_tagged_oz[100]\n",
    "print(\"POS-tagged version of the same sentence:\", witches_fate_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da995dfc",
   "metadata": {},
   "source": [
    "## Introduction to Chunking\n",
    "\n",
    "With part-of-speech tagging complete, you can now discover **patterns in sentence structure** to extract meaning. This process is called **chunking** — it groups words based on their POS tags using **regular expressions**.\n",
    "\n",
    "### What is Chunking?\n",
    "\n",
    "Chunking allows you to:\n",
    "\n",
    "* search for **structural patterns** in tagged text\n",
    "* define **custom rules** based on POS tags\n",
    "* extract meaningful **word groups** (e.g., adjective + noun)\n",
    "\n",
    "---\n",
    "\n",
    "### Chunk Grammar Format\n",
    "\n",
    "```python\n",
    "chunk_grammar = \"AN: {<JJ><NN>}\"\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* `AN` → user-defined chunk name (here: *Adjective + Noun*)\n",
    "* `{}` → chunk pattern definition\n",
    "* `<JJ>` → matches **adjective**\n",
    "* `<NN>` → matches **noun**\n",
    "\n",
    "This grammar finds **any adjective followed by a noun**.\n",
    "\n",
    "---\n",
    "\n",
    "### Running the Chunk Parser\n",
    "\n",
    "```python\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "```\n",
    "\n",
    "Once defined, you can apply it using `.parse()` on POS-tagged sentences.\n",
    "\n",
    "**Example Input:**\n",
    "\n",
    "```python\n",
    "pos_tagged_sentence = [\n",
    "    ('where', 'WRB'), ('is', 'VBZ'), \n",
    "    ('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN'), ('?', '.')\n",
    "]\n",
    "```\n",
    "\n",
    "**Example Chunking:**\n",
    "\n",
    "```python\n",
    "chunked = chunk_parser.parse(pos_tagged_sentence)\n",
    "```\n",
    "\n",
    "This identifies **“emerald city”** as a chunk — matching the `<JJ><NN>` pattern.\n",
    "\n",
    "Chunking is a powerful tool to reveal structure in text and extract meaningful information for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf9da946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked sentence showing adjective-noun pairs labeled as 'AN':\n",
      "(S ``/`` where/WRB is/VBZ the/DT (AN emerald/JJ city/NN) ?/. ''/'')\n",
      "                         S                                    \n",
      "   ______________________|__________________________           \n",
      "  |       |       |      |     |    |               AN        \n",
      "  |       |       |      |     |    |        _______|_____     \n",
      "``/`` where/WRB is/VBZ the/DT ?/. ''/'' emerald/JJ     city/NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser, Tree\n",
    "\n",
    "# Step 1: Define grammar for chunking — AN = Adjective-Noun\n",
    "# <JJ> matches adjectives and <NN> matches nouns. Together, they form small noun phrases.\n",
    "chunk_grammar = \"AN: {<JJ><NN>}\"\n",
    "\n",
    "# Step 2: Create a parser that will search for the grammar pattern in POS-tagged sentences\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# Step 3: Chunk the POS-tagged sentence at index 282 in the novel\n",
    "# This applies the grammar to detect adjective+noun pairs in that specific sentence.\n",
    "scaredy_cat = chunk_parser.parse(pos_tagged_oz[282])\n",
    "print(\"Chunked sentence showing adjective-noun pairs labeled as 'AN':\")\n",
    "print(scaredy_cat)\n",
    "\n",
    "# Step 4: Visualize the result in a tree format for easier understanding of the structure\n",
    "Tree.fromstring(str(scaredy_cat)).pretty_print()  # Displays chunks as a syntactic tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dbdee0",
   "metadata": {},
   "source": [
    "## Chunking Noun Phrases\n",
    "\n",
    "Chunking lets you group words into meaningful units based on their **part-of-speech (POS)** tags. A very useful and common type is **NP-chunking** (noun phrase chunking), where we look for phrases that behave as a single noun in a sentence.\n",
    "\n",
    "A typical **noun phrase (NP)** structure in English often looks like:\n",
    "\n",
    "* a **determiner** `DT` (e.g., *the, a, this*)\n",
    "* followed by **zero or more adjectives** `JJ` (e.g., *wicked, new*)\n",
    "* ending with a **noun** `NN` (e.g., *witch, book*)\n",
    "\n",
    "Example POS-tagged sentence:\n",
    "\n",
    "> `[('we', 'PRP'), ('are', 'VBP'), ('so', 'RB'), ('grateful', 'JJ'), ('to', 'TO'), ('you', 'PRP'), ('for', 'IN'), ('having', 'VBG'), ('killed', 'VBN'), ('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'), ('of', 'IN'), ('the', 'DT'), ('east', 'NN'), (',', ','), ('and', 'CC'), ('for', 'IN'), ('setting', 'VBG'), ('our', 'PRP$'), ('people', 'NNS'), ('free', 'VBP'), ('from', 'IN'), ('bondage', 'NN'), ('.', '.')]`\n",
    "\n",
    "From this sentence, three noun phrases of the form **(DT) (JJ)* NN** are:\n",
    "\n",
    "* `(('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'))`\n",
    "* `(('the', 'DT'), ('east', 'NN'))`\n",
    "* `(('bondage', 'NN'))` (no determiner, no adjective, just a noun)\n",
    "\n",
    "To automatically detect such patterns, you define **chunk grammar** using a regex-like syntax over POS tags:\n",
    "\n",
    "```python\n",
    "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "```\n",
    "\n",
    "Explanation of this grammar:\n",
    "\n",
    "* `NP:` — name of the chunk (noun phrase).\n",
    "* `{ ... }` — defines the pattern to chunk.\n",
    "* `<DT>` — matches any **determiner**.\n",
    "* `?` — **optional** quantifier: 0 or 1 determiner.\n",
    "* `<JJ>` — matches any **adjective**.\n",
    "* `*` — **Kleene star**: 0 or more adjectives.\n",
    "* `<NN>` — matches any **noun** (singular or plural).\n",
    "\n",
    "Once noun phrases are chunked, you can:\n",
    "\n",
    "* Perform **frequency analysis** to find important, recurring noun phrases.\n",
    "* Use NP-chunks as **pseudo-topics** to tag and categorize documents.\n",
    "* Analyze an author’s **adjective choices** (e.g., sentiment or bias) for specific nouns.\n",
    "\n",
    "The interpretation of these NP-chunks and their frequencies depends on your goals and your understanding of the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c99a9ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('i', 'NN'),), 325), ((('dorothy', 'NN'),), 222), ((('the', 'DT'), ('scarecrow', 'NN')), 213), ((('the', 'DT'), ('lion', 'NN')), 148), ((('the', 'DT'), ('tin', 'NN')), 123), ((('woodman', 'NN'),), 112), ((('oz', 'NN'),), 86), ((('toto', 'NN'),), 73), ((('head', 'NN'),), 59), ((('the', 'DT'), ('woodman', 'NN')), 59), ((('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN')), 58), ((('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN')), 51), ((('the', 'DT'), ('witch', 'NN')), 49), ((('the', 'DT'), ('girl', 'NN')), 46), ((('the', 'DT'), ('road', 'NN')), 41), ((('room', 'NN'),), 29), ((('nothing', 'NN'),), 29), ((('the', 'DT'), ('air', 'NN')), 29), ((('the', 'DT'), ('country', 'NN')), 26), ((('the', 'DT'), ('land', 'NN')), 24), ((('a', 'DT'), ('heart', 'NN')), 24), ((('the', 'DT'), ('west', 'NN')), 23), ((('axe', 'NN'),), 23), ((('the', 'DT'), ('sun', 'NN')), 22), ((('the', 'DT'), ('little', 'JJ'), ('girl', 'NN')), 22), ((('course', 'NN'),), 22), ((('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), 21), ((('aunt', 'NN'),), 21), ((('the', 'DT'), ('house', 'NN')), 21), ((('the', 'DT'), ('door', 'NN')), 21)]\n"
     ]
    }
   ],
   "source": [
    "from utils.np_chunk_counter import np_chunk_counter\n",
    "\n",
    "# Step 1: Define noun-phrase (NP) chunk grammar\n",
    "# NP: {<DT>?<JJ>*<NN>}\n",
    "# <DT>?  → optional determiner (0 or 1)\n",
    "# <JJ>*  → zero or more adjectives\n",
    "# <NN>   → a noun (the head of the phrase)\n",
    "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# Step 2: Create a RegexpParser object using the NP grammar\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# Step 3: Create an empty list to store NP-chunked sentences\n",
    "np_chunked_oz = []\n",
    "\n",
    "# Step 4: Loop through each POS-tagged sentence and apply chunking\n",
    "for sentence in pos_tagged_oz:\n",
    "    chunked_sentence = chunk_parser.parse(sentence)\n",
    "    np_chunked_oz.append(chunked_sentence)\n",
    "\n",
    "# Step 5: Count the 30 most frequent NP chunks using np_chunk_counter\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_oz)\n",
    "\n",
    "# Step 6: Display the result\n",
    "print(most_common_np_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63b7ce",
   "metadata": {},
   "source": [
    "## Chunking Verb Phrases\n",
    "\n",
    "A verb phrase (VP) contains a **verb** along with its **objects**, **modifiers**, or **complements**. VP-chunking helps us identify patterns in actions and understand how characters behave or how actions are described in a text.\n",
    "\n",
    "### Two Common Verb Phrase Structures\n",
    "\n",
    "| Structure  | Pattern                   | Description                                   |\n",
    "| ---------- | ------------------------- | --------------------------------------------- |\n",
    "| **Form 1** | `VB → NP → (optional RB)` | Verb first, noun phrase next, optional adverb |\n",
    "| **Form 2** | `NP → VB → (optional RB)` | Noun phrase first, verb next, optional adverb |\n",
    "\n",
    "Both structures carry the **same meaning**. For example:\n",
    "\n",
    "* `('said', 'VBD'), ('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')`\n",
    "* `('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN'), ('said', 'VBD')`\n",
    "\n",
    "---\n",
    "\n",
    "### **Chunk Grammar — Form 1**\n",
    "\n",
    "```python\n",
    "chunk_grammar = \"VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}\"\n",
    "```\n",
    "\n",
    "### Explanation of Each Part\n",
    "\n",
    "| Pattern   | Meaning                                                                    |\n",
    "| --------- | -------------------------------------------------------------------------- |\n",
    "| `<VB.*>`  | Matches **any verb** (VB, VBD, VBG, VBN, VBZ…) — the `.*` allows any tense |\n",
    "| `<DT>?`   | Optional determiner (0 or 1)                                               |\n",
    "| `<JJ>*`   | Zero or more adjectives                                                    |\n",
    "| `<NN>`    | A noun (head of the phrase)                                                |\n",
    "| `<RB.?>?` | **Entire adverb is optional** — see breakdown below                        |\n",
    "\n",
    "---\n",
    "\n",
    "### **Understanding `<RB.?>?`**\n",
    "\n",
    "| Position     | Meaning                                                                  |\n",
    "| ------------ | ------------------------------------------------------------------------ |\n",
    "| `.` (inside) | Wildcard → any single character                                          |\n",
    "| `?` (inner)  | Makes that character optional → allows `RB`, `RBR`, or `RBS`             |\n",
    "| `?` (outer)  | Makes the **entire adverb optional** — it may appear once, or not at all |\n",
    "\n",
    "✔ Valid adverb forms matched:\n",
    "`RB` → regular adverb (*quickly*)\n",
    "`RBR` → comparative (*faster*)\n",
    "`RBS` → superlative (*fastest*)\n",
    "`None` → no adverb at all\n",
    "\n",
    "---\n",
    "\n",
    "### Chunk Grammar — Form 2\n",
    "\n",
    "```python\n",
    "chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "```\n",
    "\n",
    "This reverses NP and verb — but the meaning is the same.\n",
    "\n",
    "---\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "Once VP chunks are extracted, you can:\n",
    "\n",
    "* Analyze character **actions**\n",
    "* Investigate **tone/bias** through adverbs\n",
    "* Track **frequent verb phrases** for narrative insight\n",
    "\n",
    "This is where your interpretation and domain knowledge become valuable for understanding the text beyond raw syntax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce707c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('said', 'VBD'), ('the', 'DT'), ('scarecrow', 'NN')), 33), ((('said', 'VBD'), ('dorothy', 'NN')), 31), ((('asked', 'VBN'), ('dorothy', 'NN')), 20), ((('said', 'VBD'), ('the', 'DT'), ('tin', 'NN')), 19), ((('said', 'VBD'), ('the', 'DT'), ('lion', 'NN')), 15), ((('said', 'VBD'), ('the', 'DT'), ('girl', 'NN')), 10), ((('asked', 'VBN'), ('the', 'DT'), ('scarecrow', 'NN')), 10), ((('answered', 'VBD'), ('the', 'DT'), ('scarecrow', 'NN')), 8), ((('said', 'VBD'), ('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), 8), ((('said', 'VBD'), ('oz', 'NN')), 8), ((('said', 'VBD'), ('the', 'DT'), ('woodman', 'NN')), 7), ((('pass', 'VB'), ('the', 'DT'), ('night', 'NN')), 6), ((('asked', 'VBN'), ('the', 'DT'), ('girl', 'NN')), 6), ((('see', 'VB'), ('the', 'DT'), ('great', 'JJ'), ('oz', 'NN')), 6), ((('answered', 'VBD'), ('oz', 'NN')), 6), ((('replied', 'VBD'), ('oz', 'NN')), 6), ((('cried', 'VBN'), ('dorothy', 'NN')), 5), ((('asked', 'VBN'), ('the', 'DT'), ('tin', 'NN')), 5), ((('asked', 'VBN'), ('the', 'DT'), ('lion', 'NN')), 5), ((('remarked', 'VBD'), ('the', 'DT'), ('lion', 'NN')), 5), ((('answered', 'VBD'), ('dorothy', 'NN')), 5), ((('replied', 'VBD'), ('the', 'DT'), ('lion', 'NN')), 5), ((('killed', 'VBN'), ('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN')), 4), ((('said', 'VBD'), ('the', 'DT'), ('witch', 'NN')), 4), ((('replied', 'VBD'), ('the', 'DT'), ('scarecrow', 'NN')), 4), ((('answered', 'VBD'), ('the', 'DT'), ('girl', 'NN')), 4), ((('said', 'VBD'), ('the', 'DT'), ('farmer', 'NN')), 4), ((('thought', 'VBD'), ('i', 'NN')), 4), ((('answered', 'VBD'), ('the', 'DT'), ('woodman', 'NN')), 4), ((('have', 'VBP'), ('no', 'DT'), ('heart', 'NN')), 4)]\n"
     ]
    }
   ],
   "source": [
    "from utils.vp_chunk_counter import vp_chunk_counter\n",
    "\n",
    "# Step 1: Define two grammars for verb phrases (VP)\n",
    "\n",
    "# Grammar 1: Verb phrase structure → VERB + NOUN PHRASE + optional ADVERB\n",
    "# <VB.*>    → matches any verb form (VB, VBD, VBG, VBN, VBZ, etc.)\n",
    "# <DT>?     → optional determiner (0 or 1)\n",
    "# <JJ>*     → zero or more adjectives\n",
    "# <NN>      → noun (head of the noun phrase)\n",
    "# <RB.?>?   → optional adverb:\n",
    "#               inner \"?\" → RB / RBR / RBS\n",
    "#               outer \"?\" → whole adverb element is optional\n",
    "chunk_grammar1 = \"VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}\"\n",
    "\n",
    "# Step 2: Use grammar 2 for chunking (can switch between the two for comparison)\n",
    "chunk_parser = RegexpParser(chunk_grammar1)\n",
    "\n",
    "# Step 3: Empty list to store chunked sentences\n",
    "vp_chunked_oz = []\n",
    "\n",
    "# Step 4: Loop through each POS-tagged sentence and apply the chunk parser\n",
    "for sentence in pos_tagged_oz:\n",
    "    chunked_sentence = chunk_parser.parse(sentence)\n",
    "    vp_chunked_oz.append(chunked_sentence)\n",
    "\n",
    "# Step 5: Use provided function to count the 30 most common verb phrases\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_oz)\n",
    "\n",
    "# Step 6: Print results (action-based insights)\n",
    "print(most_common_vp_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6351cedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('i', 'NN'), ('am', 'VBP')), 24), ((('i', 'NN'), ('was', 'VBD')), 17), ((('dorothy', 'NN'), ('was', 'VBD')), 13), ((('i', 'NN'), ('have', 'VBP')), 9), ((('i', 'NN'), ('know', 'VBP')), 8), ((('i', 'NN'), ('had', 'VBD')), 8), ((('dorothy', 'NN'), ('had', 'VBD')), 7), ((('oz', 'NN'), ('had', 'VBD')), 7), ((('the', 'DT'), ('scarecrow', 'NN'), ('said', 'VBD')), 6), ((('i', 'NN'), ('am', 'VBP'), ('oz', 'RB')), 6), ((('oz', 'NN'), ('was', 'VBD')), 6), ((('toto', 'NN'), ('did', 'VBD'), ('not', 'RB')), 5), ((('dorothy', 'NN'), ('looked', 'VBD')), 5), ((('i', 'NN'), ('have', 'VBP'), ('never', 'RB')), 5), ((('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'), ('had', 'VBD')), 5), ((('i', 'NN'), (\"'m\", 'VBP')), 5), ((('i', 'NN'), ('do', 'VBP'), (\"n't\", 'RB')), 5), ((('i', 'NN'), ('want', 'VBP')), 5), ((('the', 'DT'), ('scarecrow', 'NN'), ('had', 'VBD')), 5), ((('the', 'DT'), ('balloon', 'NN'), ('was', 'VBD')), 4), ((('the', 'DT'), ('cyclone', 'NN'), ('had', 'VBD')), 4), ((('i', 'NN'), ('came', 'VBD')), 4), ((('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'), ('was', 'VBD')), 4), ((('dorothy', 'NN'), ('did', 'VBD'), ('not', 'RB')), 4), ((('woodman', 'NN'), ('had', 'VBD')), 4), ((('i', 'NN'), ('suppose', 'VBP')), 4), ((('the', 'DT'), ('lion', 'NN'), ('was', 'VBD')), 4), ((('toto', 'NN'), ('had', 'VBD')), 4), ((('oz', 'NN'), ('has', 'VBZ')), 4), ((('dorothy', 'NN'), ('found', 'VBD')), 3)]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define two grammars for verb phrases (VP)\n",
    "\n",
    "# Grammar 1: Verb phrase structure → VERB + NOUN PHRASE + optional ADVERB\n",
    "# <VB.*>    → matches any verb form (VB, VBD, VBG, VBN, VBZ, etc.)\n",
    "# <DT>?     → optional determiner (0 or 1)\n",
    "# <JJ>*     → zero or more adjectives\n",
    "# <NN>      → noun (head of the noun phrase)\n",
    "# <RB.?>?   → optional adverb:\n",
    "#               inner \"?\" → RB / RBR / RBS\n",
    "#               outer \"?\" → whole adverb element is optional\n",
    "# Grammar 2: Same elements, but noun phrase comes before the verb\n",
    "chunk_grammar2 = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "\n",
    "# Step 2: Use grammar 2 for chunking (can switch between the two for comparison)\n",
    "chunk_parser = RegexpParser(chunk_grammar2)\n",
    "\n",
    "# Step 3: Empty list to store chunked sentences\n",
    "vp_chunked_oz = []\n",
    "\n",
    "# Step 4: Loop through each POS-tagged sentence and apply the chunk parser\n",
    "for sentence in pos_tagged_oz:\n",
    "    chunked_sentence = chunk_parser.parse(sentence)\n",
    "    vp_chunked_oz.append(chunked_sentence)\n",
    "\n",
    "# Step 5: Use provided function to count the 30 most common verb phrases\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_oz)\n",
    "\n",
    "# Step 6: Print results (action-based insights)\n",
    "print(most_common_vp_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0149c9",
   "metadata": {},
   "source": [
    "## Chunk Filtering\n",
    "\n",
    "Chunk filtering allows you to **remove unwanted parts of speech** from chunks instead of directly defining what to include. Rather than specifying the exact structure (like NP or VP), you can chunk the **entire sentence**, then filter out certain patterns that break the phrase.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "This method chunks the full sentence first, and then **filters out specific POS tags** such as verbs or prepositions. If a filtered tag appears **in the middle of a chunk**, it will **split the chunk** into two separate pieces — helping isolate meaningful phrases.\n",
    "\n",
    "### Chunk Filtering Grammar\n",
    "\n",
    "```python\n",
    "chunk_grammar = \"\"\"NP: {<.*>+}\n",
    "                       }<VB.?|IN>+{\"\"\"\n",
    "```\n",
    "\n",
    "### Explanation of the Grammar\n",
    "\n",
    "| Component | Purpose                                               |                                                                                              |\n",
    "| --------- | ----------------------------------------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| `NP:`     | Defines the chunk name (noun phrase).                 |                                                                                              |\n",
    "| `{<.*>+}` | Chunks **everything** in the sentence (all POS tags). |                                                                                              |\n",
    "| `}<VB.?   | IN>+{`                                                | Filters out **verbs (`VB.*`) and prepositions (`IN`)**. These split chunks when encountered. |\n",
    "\n",
    "### Why Use Chunk Filtering?\n",
    "\n",
    "* Helps **extract meaningful segments** without strict grammatical rules.\n",
    "* Reveals phrases where nouns naturally group together.\n",
    "* Can be used to **remove noise** while keeping the functional phrase.\n",
    "\n",
    "Chunk filtering gives you a *more flexible* way to explore patterns in POS-tagged text — especially useful when sentence structures vary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9beead6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One big chunk: (S\n",
      "  (Chunk\n",
      "    then/RB\n",
      "    she/PRP\n",
      "    sat/VBD\n",
      "    upon/IN\n",
      "    a/DT\n",
      "    settee/NN\n",
      "    and/CC\n",
      "    watched/VBD\n",
      "    the/DT\n",
      "    people/NNS\n",
      "    dance/NN\n",
      "    ./.))\n",
      "Filtered NP chunks: (S\n",
      "  (NP then/RB she/PRP)\n",
      "  sat/VBD\n",
      "  upon/IN\n",
      "  (NP a/DT settee/NN and/CC)\n",
      "  watched/VBD\n",
      "  (NP the/DT people/NNS dance/NN ./.))\n",
      "                                                 S                                                  \n",
      "    _____________________________________________|_______________________________                    \n",
      "   |       |         |               NP                  NP                      NP                 \n",
      "   |       |         |          _____|_____       _______|_______        ________|________________   \n",
      "sat/VBD upon/IN watched/VBD then/RB     she/PRP a/DT settee/NN and/CC the/DT people/NNS dance/NN ./.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Chunk entire sentence into one big chunk\n",
    "grammar = \"Chunk: {<.*>+}\"\n",
    "parser = RegexpParser(grammar)\n",
    "chunked_dancers = parser.parse(pos_tagged_oz[230])\n",
    "print(\"One big chunk:\", chunked_dancers)\n",
    "\n",
    "# Step 2: Define chunk filtering grammar (NP = noun phrase)\n",
    "chunk_grammar = \"\"\"NP: {<.*>+}\n",
    "                       }<VB.?|IN>+{\"\"\"\n",
    "\n",
    "# Step 3: Create parser using NP filtering grammar\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# Step 4: Apply filtering to sentence at index 230\n",
    "filtered_dancers = chunk_parser.parse(pos_tagged_oz[230])\n",
    "print(\"Filtered NP chunks:\", filtered_dancers)\n",
    "\n",
    "# Step 5: Visualize the filtered chunks\n",
    "Tree.fromstring(str(filtered_dancers)).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa37d7",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "You now have the tools to perform natural language parsing using **regular expressions** and **nltk**. With these techniques, you can uncover structure, meaning, and even bias in any text dataset.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* **Regex Methods**\n",
    "\n",
    "  * `.compile()` and `.match()` → look for a **single match at the start** of a string.\n",
    "  * `.search()` → finds the **first match anywhere** in the string.\n",
    "  * `.findall()` → returns **all non-overlapping matches**.\n",
    "\n",
    "* **Part-of-Speech Tagging**\n",
    "\n",
    "  * Use `pos_tag()` in nltk to label each word by its grammatical role.\n",
    "\n",
    "* **Chunking**\n",
    "\n",
    "  * Group words based on POS tags using a regex-style **chunk grammar**.\n",
    "  * Use `RegexpParser(...).parse()` to apply chunking to tokenized sentences.\n",
    "\n",
    "* **NP-Chunking (Noun Phrases)**\n",
    "\n",
    "  * Pattern: optional `DT` + any number of `JJ` + `NN`\n",
    "  * Useful for finding recurring **subjects and topics** in a text.\n",
    "\n",
    "* **VP-Chunking (Verb Phrases)**\n",
    "\n",
    "  * Pattern: verb `VB` + noun phrase + optional adverb `RB`\n",
    "  * Reveals how actions are described and how subjects behave — helpful for detecting **bias or attitude**.\n",
    "\n",
    "* **Chunk Filtering**\n",
    "\n",
    "  * Instead of specifying what to include, specify what to **remove** to isolate meaningful chunks.\n",
    "\n",
    "These tools form the foundation of many NLP preprocessing pipelines — and now you can build your own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76175cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('i', 'NN'),), 293), ((('the', 'DT'), ('scarecrow', 'NN')), 212), ((('dorothy', 'NN'),), 211), ((('the', 'DT'), ('lion', 'NN')), 147), ((('the', 'DT'), ('tin', 'NN'), ('woodman', 'NN')), 111), ((('oz', 'NN'),), 84), ((('toto', 'NN'),), 73), ((('the', 'DT'), ('woodman', 'NN')), 59), ((('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN')), 58), ((('head', 'NN'),), 56), ((('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN')), 49), ((('the', 'DT'), ('witch', 'NN')), 47), ((('i', 'NNS'),), 46), ((('the', 'DT'), ('girl', 'NN')), 44), ((('the', 'DT'), ('road', 'NN')), 41), ((('brains', 'NNS'),), 34), ((('eyes', 'NNS'),), 31), ((('arms', 'NNS'),), 29), ((('the', 'DT'), ('air', 'NN')), 29), ((('nothing', 'NN'),), 28), ((('friends', 'NNS'),), 28), ((('the', 'DT'), ('country', 'NN')), 26), ((('the', 'DT'), ('people', 'NNS')), 25), ((('the', 'DT'), ('winkies', 'NNS')), 25), ((('the', 'DT'), ('land', 'NN')), 24), ((('a', 'DT'), ('heart', 'NN')), 24), ((('the', 'DT'), ('west', 'NN')), 23), ((('the', 'DT'), ('little', 'JJ'), ('girl', 'NN')), 22), ((('axe', 'NN'),), 22), ((('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), 21)]\n"
     ]
    }
   ],
   "source": [
    "from utils.chunk_counter import chunk_counter\n",
    "\n",
    "# custom chunk grammar: focus on descriptive noun clusters and remove noise\n",
    "chunk_grammar = r'''\n",
    "Chunk: {<DT>?<JJ>*<NN.*>+}\n",
    "            }<VB.*|IN|CC|PRP|PRP\\$>+{\n",
    "'''\n",
    "\n",
    "# create parser\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# list to hold chunked sentences\n",
    "chunked_oz = []\n",
    "\n",
    "# loop through each POS-tagged sentence and apply chunking\n",
    "for pos_tagged_sentence in pos_tagged_oz:\n",
    "    chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "# count and print most common chunks\n",
    "most_common_chunks = chunk_counter(chunked_oz)\n",
    "print(most_common_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5b3cf",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Text preprocessing is the process of cleaning and preparing text data for use in a specific context. It is used in almost all **NLP pipelines**, including:\n",
    "\n",
    "* voice recognition software\n",
    "* search engine lookups\n",
    "* machine learning model training\n",
    "\n",
    "Text data can vary widely—from its format (websites, messages, voice input) to the people generating it (language, dialect). These differences introduce **noise**, which must be reduced.\n",
    "\n",
    "## Goal\n",
    "\n",
    "The main objective is to keep **only the words needed** for your NLP task.\n",
    "\n",
    "## What You Will Learn\n",
    "\n",
    "This lesson introduces key strategies for preparing text data. While not exhaustive, it includes several widely used techniques:\n",
    "\n",
    "* **Using Regex & NLTK libraries**\n",
    "* **Noise Removal** — removing unwanted characters and formatting\n",
    "* **Tokenization** — splitting text into smaller units called *tokens*\n",
    "* **Normalization** — broader processing tasks including **stemming** and **lemmatization**\n",
    "\n",
    "These methods form the foundation of most NLP workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785950d5",
   "metadata": {},
   "source": [
    "**In anutshell: how does it work**\n",
    "\n",
    "Original Sentence\n",
    "\n",
    "\"Who was partying?\"\n",
    "\n",
    "Step 1 – Noise Removal: Remove punctuation and special characters:\n",
    "\n",
    "\"Who was partying\"\n",
    "\n",
    "Step 2 – Lowercasing: Convert all text to lowercase for consistency:\n",
    "\n",
    "\"who was partying\"\n",
    "\n",
    "Step 3 – Tokenization: Split the sentence into individual units (tokens):\n",
    "\n",
    "[\"who\", \"was\", \"partying\"]\n",
    "\n",
    "Step 4 – Lemmatization: Convert each token to its base/dictionary form:\n",
    "\n",
    "[\"who\", \"be\", \"party\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79522df8",
   "metadata": {},
   "source": [
    "## Text Preprocessing — Noise Removal\n",
    "\n",
    "Text cleaning is a crucial step in preparing data for NLP tasks. Raw text often contains **noise** that does not contribute to your analysis and must be removed beforehand.\n",
    "\n",
    "### Common Types of Noise\n",
    "\n",
    "Depending on your data source (Twitter API, web scraping, voice input, etc.), you may need to remove:\n",
    "\n",
    "* Punctuation and accents\n",
    "* Special characters\n",
    "* Numeric digits\n",
    "* Leading/trailing/vertical whitespace\n",
    "* HTML formatting\n",
    "\n",
    "---\n",
    "\n",
    "### Using `re.sub()` for Noise Removal\n",
    "\n",
    "The `.sub()` method from Python’s `re` module is commonly used for text cleaning.\n",
    "\n",
    "**It takes three required arguments:**\n",
    "\n",
    "| Argument           | Description                                 |\n",
    "| ------------------ | ------------------------------------------- |\n",
    "| `pattern`          | Regex to search for (use `r\"\"` raw strings) |\n",
    "| `replacement_text` | Text to replace matches with                |\n",
    "| `input`            | Original string being cleaned               |\n",
    "\n",
    "**Returns:** a new string with replacements applied.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 1 — Removing HTML Tags\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "text = \"<p>    This is a paragraph</p>\"\n",
    "result = re.sub(r'<.?p>', '', text)\n",
    "print(result)\n",
    "#    This is a paragraph\n",
    "```\n",
    "\n",
    "Here, we replace the tags with an **empty string** `''`, effectively removing them.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2 — Removing Whitespace\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "text = \"    This is a paragraph\"\n",
    "result = re.sub(r'\\s{4}', '', text)\n",
    "print(result)\n",
    "# This is a paragraph\n",
    "```\n",
    "\n",
    "This removes exactly four spaces at the beginning of the string.\n",
    "\n",
    "---\n",
    "\n",
    "These tools allow you to clean raw text efficiently and prepare it for tokenization, normalization, or deeper NLP processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbcc204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nation's Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini\n",
      "fat_meats, veggies are better than you think.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import regular expression library\n",
    "import re\n",
    "\n",
    "# Step 2: Remove opening and closing <h1> tags from the headline\n",
    "headline_one = '<h1>Nation\\'s Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini</h1>'\n",
    "headline_no_tag = re.sub(r\"<.?h1>\", \"\", headline_one)  # removes both <h1> and </h1> tags\n",
    "\n",
    "# Step 3: Remove all '@' characters from the tweet\n",
    "tweet = '@fat_meats, veggies are better than you think.'\n",
    "tweet_no_at = re.sub(r\"@\", \"\", tweet)  # removes @ symbols\n",
    "\n",
    "# Test prints\n",
    "try:\n",
    "    print(headline_no_tag)\n",
    "except:\n",
    "    print('No variable called `headline_no_tag`')\n",
    "\n",
    "try:\n",
    "    print(tweet_no_at)\n",
    "except:\n",
    "    print('No variable called `tweet_no_at`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe18bb97",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "To perform most NLP tasks, we first need to **break text into smaller components**. This process is called **tokenization**, and the resulting pieces are called **tokens**.\n",
    "\n",
    "### Why Tokenize?\n",
    "\n",
    "Tokenization helps with tasks such as:\n",
    "\n",
    "* Counting words or sentences\n",
    "* Tracking word frequency\n",
    "* Finding co-occurring terms\n",
    "* Preparing text for models and analysis\n",
    "\n",
    "Tokens are usually **individual words**, but they can also be **sentences** or other structured units.\n",
    "\n",
    "---\n",
    "\n",
    "### Word Tokenization\n",
    "\n",
    "Use `word_tokenize()` from **nltk** to split text into individual words:\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenize this text\"\n",
    "tokenized = word_tokenize(text)\n",
    "\n",
    "print(tokenized)\n",
    "# [\"Tokenize\", \"this\", \"text\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Sentence Tokenization\n",
    "\n",
    "Use `sent_tokenize()` when you want to separate entire sentences:\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Tokenize this sentence. Also, tokenize this sentence.\"\n",
    "tokenized = sent_tokenize(text)\n",
    "\n",
    "print(tokenized)\n",
    "# ['Tokenize this sentence.', 'Also, tokenize this sentence.']\n",
    "```\n",
    "\n",
    "Tokenization is often the **first step** in NLP preprocessing — it provides access to each part of the text for further analysis or transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ef11f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:\n",
      "['An', 'electrocardiogram', 'is', 'used', 'to', 'record', 'the', 'electrical', 'conduction', 'through', 'a', 'person', \"'s\", 'heart', '.', 'The', 'readings', 'can', 'be', 'used', 'to', 'diagnose', 'cardiac', 'arrhythmias', '.']\n",
      "Sentence Tokenization:\n",
      "[\"An electrocardiogram is used to record the electrical conduction through a person's heart.\", 'The readings can be used to diagnose cardiac arrhythmias.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Sample ECG text\n",
    "ecg_text = 'An electrocardiogram is used to record the electrical conduction through a person\\'s heart. The readings can be used to diagnose cardiac arrhythmias.'\n",
    "\n",
    "# Step 1: Tokenize by word\n",
    "tokenized_by_word = word_tokenize(ecg_text)\n",
    "\n",
    "# Step 2: Tokenize by sentence\n",
    "tokenized_by_sentence = sent_tokenize(ecg_text)\n",
    "\n",
    "# Print results\n",
    "try:\n",
    "    print('Word Tokenization:')\n",
    "    print(tokenized_by_word)\n",
    "except:\n",
    "    print('Expected a variable called `tokenized_by_word`')\n",
    "\n",
    "try:\n",
    "    print('Sentence Tokenization:')\n",
    "    print(tokenized_by_sentence)\n",
    "except:\n",
    "    print('Expected a variable called `tokenized_by_sentence`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b239201",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "After tokenization and noise removal, many NLP workflows require **text normalization**, which prepares tokens for deeper analysis. Normalization includes several types of preprocessing:\n",
    "\n",
    "### Common Normalization Techniques\n",
    "\n",
    "* **Upper or lowercasing**\n",
    "* **Stopword removal**\n",
    "* **Stemming** — removing prefixes/suffixes\n",
    "* **Lemmatization** — converting words to their root form\n",
    "\n",
    "The simplest normalization step is **changing text to lowercase or uppercase**, ensuring consistency across tokens.\n",
    "\n",
    "### Example — Changing Case\n",
    "\n",
    "```python\n",
    "my_string = 'tHiS HaS a MiX oF cAsEs'\n",
    "\n",
    "print(my_string.upper())\n",
    "# 'THIS HAS A MIX OF CASES'\n",
    "\n",
    "print(my_string.lower())\n",
    "# 'this has a mix of cases'\n",
    "```\n",
    "\n",
    "Normalization helps reduce variation in text (e.g., “Apple”, “apple”, “APPLE”) and improves token consistency for downstream NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a76e9b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased brands: salvation army, ymca, boys & girls club of america\n",
      "Uppercased brands: SALVATION ARMY, YMCA, BOYS & GIRLS CLUB OF AMERICA\n"
     ]
    }
   ],
   "source": [
    "brands = 'Salvation Army, YMCA, Boys & Girls Club of America'\n",
    "\n",
    "# Step 1: Convert all characters to lowercase\n",
    "brands_lower = brands.lower()\n",
    "\n",
    "# Step 2: Convert all characters to uppercase\n",
    "brands_upper = brands.upper()\n",
    "\n",
    "# Print to verify results\n",
    "try:\n",
    "    print(f'Lowercased brands: {brands_lower}')\n",
    "except:\n",
    "    print('Expected a variable called `brands_lower`')\n",
    "\n",
    "try:\n",
    "    print(f'Uppercased brands: {brands_upper}')\n",
    "except:\n",
    "    print('Expected a variable called `brands_upper`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e90fd3",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "\n",
    "Stopwords are high-frequency words that often provide **little to no meaning** in a sentence. They include words like **“a”**, **“an”**, **“the”**, and other common terms that do not affect sentiment or intent. Removing them helps focus on meaningful words.\n",
    "\n",
    "### Using NLTK’s Stopword List\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "```\n",
    "\n",
    "The words are stored as a **set** for fast lookup.\n",
    "\n",
    "---\n",
    "\n",
    "### Removing Stopwords from Text\n",
    "\n",
    "```python\n",
    "nbc_statement = \"NBC was founded in 1926 making it the oldest major broadcast network in the USA\"\n",
    "\n",
    "word_tokens = word_tokenize(nbc_statement)  # tokenize sentence\n",
    "\n",
    "statement_no_stop = [word for word in word_tokens if word not in stop_words]\n",
    "\n",
    "print(statement_no_stop)\n",
    "# ['NBC', 'founded', '1926', 'making', 'oldest', 'major', 'broadcast', 'network', 'USA']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Process Summary:**\n",
    "\n",
    "1. Tokenize the text.\n",
    "2. Filter out words that are in `stop_words`.\n",
    "3. Keep only the meaningful words for analysis.\n",
    "\n",
    "Stopword removal is essential for focusing on **semantically relevant content** during text processing and NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6d41167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords type: <class 'set'>\n",
      "Words Tokenized: ['A', 'YouGov', 'study', 'found', 'that', 'American', \"'s\", 'like', 'Italian', 'food', 'more', 'than', 'any', 'other', 'country', \"'s\", 'cuisine', '.']\n",
      "Text without Stops: ['A', 'YouGov', 'study', 'found', 'American', \"'s\", 'like', 'Italian', 'food', 'country', \"'s\", 'cuisine', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Sample survey text\n",
    "survey_text = 'A YouGov study found that American\\'s like Italian food more than any other country\\'s cuisine.'\n",
    "\n",
    "# Step 1: Load English stopwords into a set\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Step 2: Tokenize the survey text\n",
    "tokenized_survey = word_tokenize(survey_text)\n",
    "\n",
    "# Step 3: Remove stopwords using list comprehension\n",
    "text_no_stops = [word for word in tokenized_survey if word not in stop_words]\n",
    "\n",
    "# Print results for verification\n",
    "try:\n",
    "    print(f'Stopwords type: {type(stop_words)}')\n",
    "except:\n",
    "    print('Expected a variable called `stop_words`')\n",
    "\n",
    "try:\n",
    "    print(f'Words Tokenized: {tokenized_survey}')\n",
    "except:\n",
    "    print('Expected a variable called `tokenized_survey`')\n",
    "\n",
    "try:\n",
    "    print(f'Text without Stops: {text_no_stops}')\n",
    "except:\n",
    "    print('Expected a variable called `text_no_stops`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c6ea1",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is a normalization technique used in NLP to **remove prefixes and suffixes** from words, reducing them to a simpler base form. This helps unify similar words such as:\n",
    "\n",
    "* *going* → **go**\n",
    "* *connected* → **connect**\n",
    "* *happiness* → **happi**\n",
    "\n",
    "Search engines often use stemming to improve keyword matching between queries and documents.\n",
    "\n",
    "---\n",
    "\n",
    "### Using `PorterStemmer` in NLTK\n",
    "\n",
    "To use stemming, first import and initialize the stemmer:\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Applying Stemming to Tokenized Text\n",
    "\n",
    "```python\n",
    "tokenized = ['NBC', 'was', 'founded', 'in', '1926', '.', 'This', 'makes', 'NBC', 'the', 'oldest', 'major', 'broadcast', 'network', '.']\n",
    "\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "\n",
    "print(stemmed)\n",
    "# ['nbc', 'wa', 'found', 'in', '1926', '.', 'thi', 'make', 'nbc', 'the', 'oldest', 'major', 'broadcast', 'network', '.']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Important Note\n",
    "\n",
    "Stemming is powerful but **not always precise** — words may become shortened to forms that no longer resemble their original meaning. Use it carefully depending on your NLP task.\n",
    "\n",
    "Stemming is often followed by **lemmatization** for more accurate normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e841cd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A stemmer exists:\n",
      "<PorterStemmer>\n",
      "Words Tokenized:\n",
      "['Java', 'is', 'an', 'Indonesian', 'island', 'in', 'the', 'Pacific', 'Ocean', '.', 'It', 'is', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'people', '.']\n",
      "Stemmed Words:\n",
      "['java', 'is', 'an', 'indonesian', 'island', 'in', 'the', 'pacif', 'ocean', '.', 'it', 'is', 'the', 'most', 'popul', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'peopl', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import PorterStemmer and initialize it\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Step 2: Tokenize the text\n",
    "populated_island = 'Java is an Indonesian island in the Pacific Ocean. It is the most populated island in the world, with over 140 million people.'\n",
    "island_tokenized = word_tokenize(populated_island)\n",
    "\n",
    "# Step 3: Apply stemming using a list comprehension\n",
    "stemmed = [stemmer.stem(token) for token in island_tokenized]\n",
    "\n",
    "# Print results for verification\n",
    "try:\n",
    "    print('A stemmer exists:')\n",
    "    print(stemmer)\n",
    "except:\n",
    "    print('Expected a variable called `stemmer`')\n",
    "\n",
    "try:\n",
    "    print('Words Tokenized:')\n",
    "    print(island_tokenized)\n",
    "except:\n",
    "    print('Expected a variable called `island_tokenized`')\n",
    "\n",
    "try:\n",
    "    print('Stemmed Words:')\n",
    "    print(stemmed)\n",
    "except:\n",
    "    print('Expected a variable called `stemmed`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b13a3",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "**Lemmatization** is a normalization technique that converts words to their **root (dictionary) form**, known as the *lemma*. Unlike stemming, lemmatization considers the **part of speech (POS)** of each word, making it **more accurate but less efficient**.\n",
    "\n",
    "---\n",
    "\n",
    "### Using `WordNetLemmatizer` in NLTK\n",
    "\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Applying Lemmatization to Tokens\n",
    "\n",
    "```python\n",
    "tokenized = [\"NBC\", \"was\", \"founded\", \"in\", \"1926\"]\n",
    "\n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "\n",
    "print(lemmatized)\n",
    "# [\"NBC\", \"wa\", \"founded\", \"in\", \"1926\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why “was” → “wa”?\n",
    "\n",
    "Because `lemmatize()` **assumes every word is a noun by default**.\n",
    "To get accurate results (e.g., “was” → “be”), we must supply **part-of-speech tags** — which we'll explore in the next exercise.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Difference vs. Stemming:**\n",
    "\n",
    "| Feature        | Stemming           | Lemmatization         |\n",
    "| -------------- | ------------------ | --------------------- |\n",
    "| Considers POS? | ❌ No               | ✔ Yes                 |\n",
    "| Accuracy       | Low–Medium         | High                  |\n",
    "| Output         | Can be nonsensical | Valid dictionary word |\n",
    "\n",
    "Lemmatization is most useful when **POS tagging** is available — then it becomes a powerful tool for NLP preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38df910f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lemmatizer exists: <WordNetLemmatizer>\n",
      "Words Tokenized: ['Indonesia', 'was', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n",
      "Lemmatized Words: ['Indonesia', 'wa', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Step 1: Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 2: Tokenize the input string\n",
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "\n",
    "# Step 3: Lemmatize each token using list comprehension\n",
    "lemmatized_words = [lemmatizer.lemmatize(token) for token in tokenized_string]\n",
    "\n",
    "# Print results for verification\n",
    "try:\n",
    "    print(f'A lemmatizer exists: {lemmatizer}')\n",
    "except:\n",
    "    print('Expected a variable called `lemmatizer`')\n",
    "try:\n",
    "    print(f'Words Tokenized: {tokenized_string}')\n",
    "except:\n",
    "    print('Expected a variable called `tokenized_string`')\n",
    "try:\n",
    "    print(f'Lemmatized Words: {lemmatized_words}')\n",
    "except:\n",
    "    print('Expected a variable called `lemmatized_words`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8683d3d",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging\n",
    "\n",
    "To improve lemmatization accuracy, we first need to determine the **part of speech (POS)** for each word in a string. This allows us to provide contextual information to the lemmatizer, resulting in more accurate word roots.\n",
    "\n",
    "### How the POS Tagging Function Works\n",
    "\n",
    "#### 1. Import Required Tools\n",
    "\n",
    "```python\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "```\n",
    "\n",
    "* **wordnet** provides word meanings and POS categories\n",
    "* **Counter** counts occurrences of each POS type\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Get Synonyms (Word Senses)\n",
    "\n",
    "```python\n",
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "```\n",
    "\n",
    "`synsets()` returns possible meanings of a word — each tagged with a POS.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Count POS Types\n",
    "\n",
    "We count how many times each part of speech appears:\n",
    "\n",
    "```python\n",
    "pos_counts[\"n\"] = len([item for item in probable_part_of_speech if item.pos() == \"n\"])  # nouns\n",
    "# ... same for verbs, adjectives, adverbs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Determine the Most Likely POS\n",
    "\n",
    "```python\n",
    "most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "```\n",
    "\n",
    "This returns the most probable part of speech for the input word.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Improved Lemmatization\n",
    "\n",
    "```python\n",
    "tokenized = [\"How\", \"old\", \"is\", \"the\", \"country\", \"Indonesia\"]\n",
    "\n",
    "lemmatized = [\n",
    "    lemmatizer.lemmatize(token, get_part_of_speech(token)) \n",
    "    for token in tokenized\n",
    "]\n",
    "\n",
    "print(lemmatized)\n",
    "# ['How', 'old', 'be', 'the', 'country', 'Indonesia']\n",
    "```\n",
    "\n",
    "Because the correct POS was provided, **“is” became “be”** — something basic lemmatization could not achieve.\n",
    "\n",
    "---\n",
    "\n",
    "This approach provides **context-aware normalization**, essential for accurate NLP preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55a26264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized words are: ['Indonesia', 'be', 'found', 'in', '1945', '.', 'It', 'contain', 'the', 'most', 'populate', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from utils.part_of_speech2 import get_part_of_speech\n",
    "\n",
    "# Step 1: Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 2: Tokenize the input string\n",
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "\n",
    "# Step 3: Lemmatize using POS tagging for better accuracy\n",
    "lemmatized_pos = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized_string]\n",
    "\n",
    "# Print results\n",
    "try:\n",
    "    print(f'The lemmatized words are: {lemmatized_pos}')\n",
    "except:\n",
    "    print('Expected a variable called `lemmatized_pos`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9f7ca",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "This lesson introduced essential techniques for **text preprocessing**, a crucial step before applying NLP models or analysis. The goal is to prepare raw text into a clean and structured form suitable for downstream tasks.\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "* **Text Preprocessing**\n",
    "  The process of cleaning and formatting text so it is usable for NLP applications.\n",
    "\n",
    "* **Noise Removal**\n",
    "  Removes irrelevant elements such as punctuation, HTML tags, symbols, or excess whitespace.\n",
    "\n",
    "* **Tokenization**\n",
    "  Splits text into smaller units — typically **words** or **sentences** — using tools like `word_tokenize()` and `sent_tokenize()`.\n",
    "\n",
    "* **Normalization**\n",
    "  A set of transformations that makes tokens more consistent:\n",
    "\n",
    "  * Uppercasing / Lowercasing\n",
    "  * Stopword removal\n",
    "  * **Stemming** — bluntly removes prefixes/suffixes\n",
    "  * **Lemmatization** — finds the dictionary root of a word\n",
    "\n",
    "* **Stemming**\n",
    "  Reduces words to shorter forms (may lose meaning).\n",
    "  Example: “running” → “run”, “was” → “wa”\n",
    "\n",
    "* **Lemmatization**\n",
    "  More accurate than stemming and often uses **part-of-speech tagging** to return meaningful word roots.\n",
    "  Example: “was” → “be”\n",
    "\n",
    "---\n",
    "\n",
    "### Final Takeaway\n",
    "\n",
    "Before building your preprocessing pipeline, **decide how you want the text formatted** and **why**. Once your goal is clear, these preprocessing tools allow you to shape the data exactly as needed for NLP models, analysis, or feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3aa4b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'further', \"they'd\", 'same', 'did', \"that'll\", 'd', \"i've\", \"needn't\", 'only', 'very', 'that', 'not', 'and', 'some', 'a', \"we'll\", 'yours', 'can', \"she's\", \"we're\", \"shan't\", 'for', \"she'd\", 'needn', 'in', 'mightn', 'so', 'll', 'me', 'is', 'more', 'there', 'doing', \"you'll\", 'then', \"it's\", \"he's\", 'm', 'but', 'between', 'hasn', 'itself', \"they've\", \"i'm\", 'no', 'both', 'you', \"hasn't\", 'through', 'again', 'as', 'during', 'its', 'themselves', \"aren't\", 'him', 'an', 'shouldn', 'where', 'being', 'while', 'weren', 'against', 'how', 'should', 'who', 'few', 'out', 'are', 'whom', \"you're\", 'these', 've', 'of', 'will', 'below', 'over', 'now', 'this', 'all', \"shouldn't\", 'what', 'haven', \"it'll\", 'down', \"i'd\", 'under', 'with', \"you've\", 'didn', 'most', \"it'd\", 't', 'your', 'about', 'do', 'they', 'too', 'her', 'does', 'here', 'their', 'doesn', 'won', 'own', 're', 'theirs', \"couldn't\", 's', 'each', 'mustn', 'because', 'other', 'were', 'don', 'hers', \"don't\", 'hadn', 'the', 'wouldn', 'than', 'after', 'to', 'was', \"hadn't\", 'i', 'wasn', 'which', 'my', \"i'll\", 'ours', 'been', \"wouldn't\", \"he'll\", 'she', 'yourself', 'yourselves', 'just', 'on', \"isn't\", \"they're\", 'aren', 'until', 'isn', 'our', 'at', 'into', 'it', 'above', \"didn't\", 'once', \"they'll\", 'myself', \"mustn't\", \"she'll\", 'any', 'himself', 'if', 'ma', 'such', 'those', 'has', 'have', \"should've\", 'we', \"won't\", \"wasn't\", 'ain', 'off', \"haven't\", 'up', 'when', 'couldn', 'ourselves', 'y', 'he', 'herself', 'am', \"you'd\", 'by', 'nor', 'his', 'be', \"we'd\", 'had', 'why', 'o', \"weren't\", 'before', 'them', \"mightn't\", \"we've\", 'having', \"he'd\", 'or', 'shan', 'from', \"doesn't\"}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from utils.part_of_speech import get_part_of_speech\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Step 1: Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 2: Original HTML text\n",
    "oprah_wiki = \"<p>Working in local media, she was both the youngest news anchor and the first black female news anchor at Nashville's WLAC-TV. </p>\"\n",
    "\n",
    "# Step 3: Remove <p> tags\n",
    "cleaned_wiki = re.sub(r\"<.?p>\", \"\", oprah_wiki)\n",
    "\n",
    "# Step 4: Remove periods and commas using regex\n",
    "cleaned_wiki = re.sub(r\"[.,]\", \"\", cleaned_wiki)\n",
    "\n",
    "# Step 5: Convert to lowercase\n",
    "cleaned_wiki = cleaned_wiki.lower()\n",
    "\n",
    "# Step 6: Tokenize\n",
    "tokenized_sentence = word_tokenize(cleaned_wiki)\n",
    "\n",
    "# Step 7: Remove stopwords (including punctuation tokens)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4eaaaa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'local', 'medium', 'young', 'news', 'anchor', 'first', 'black', 'female', 'news', 'anchor', 'nashville', 'wlac-tv']\n"
     ]
    }
   ],
   "source": [
    "stop_words.update({\"''\", \"``\", \"'s\"})  # extra cleanup\n",
    "text_no_stops = [word for word in tokenized_sentence if word not in stop_words]\n",
    "\n",
    "# Step 8: Lemmatize using POS tagging\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, get_part_of_speech(word)) for word in text_no_stops]\n",
    "\n",
    "# Final output\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc707a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

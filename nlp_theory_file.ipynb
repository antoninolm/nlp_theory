{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2d500d",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size: 28px;\"><b>NLP Basics</b></p>\n",
    "\n",
    "\n",
    "This notebook summarizes the key concepts explored throughout the **Natural Language Processing (NLP)** lessons.\n",
    "It provides an overview of foundational methods, models, and ethical considerations that define the field.\n",
    "\n",
    "### What’s Inside\n",
    "\n",
    "* **Foundations of NLP** – bridging linguistics, computer science, and AI\n",
    "* **Text Preprocessing** – cleaning, tokenization, normalization\n",
    "* **Parsing & Syntax** – analyzing grammatical structure\n",
    "* **Language Models** – Bag-of-Words, n-grams, and LSTMs\n",
    "* **Topic Modeling & Similarity** – identifying themes and measuring relatedness\n",
    "* **Language Prediction** – generating text and suggestions\n",
    "* **Ethical Considerations** – addressing bias and privacy in NLP systems\n",
    "\n",
    "Use this notebook to revisit core ideas and reinforce your understanding of how NLP enables machines to process and generate human language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dbca01",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "Look around at the technologies we use every day:\n",
    "\n",
    "* **Spellcheck and autocorrect**\n",
    "* **Auto-generated video captions**\n",
    "* **Virtual assistants** like Amazon’s Alexa\n",
    "* **Autocomplete**\n",
    "* **News site recommendations**\n",
    "\n",
    "What do they all have in common?\n",
    "They rely on **Natural Language Processing (NLP)** — a field at the intersection of **linguistics**, **artificial intelligence**, and **computer science**.\n",
    "\n",
    "The goal of NLP is to enable computers to **interpret**, **analyze**, and **generate** human languages such as English or Spanish.\n",
    "\n",
    "NLP’s origins trace back to around **1950**, when **Alan Turing** proposed his famous **Turing Test** — a way to evaluate whether a computer can use language convincingly enough to make humans believe it’s human.\n",
    "\n",
    "Today, NLP powers a wide range of applications beyond speech approximation — from **detecting spam emails** and **analyzing bias in tweets** to **improving accessibility** for people with disabilities.\n",
    "\n",
    "While many programming languages can be used for NLP, **Python** stands out thanks to its extensive open-source libraries such as **NLTK (Natural Language Toolkit)**.\n",
    "In this module, you’ll use **Python** to get your **first hands-on experience with NLP**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e0ec44",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "> \"You never know what you have... until you clean your data.\"\n",
    "> ~ Unknown (or possibly made up)\n",
    "\n",
    "Before diving into model training or sentiment analysis, you must **clean and prepare your text data** — and this process is known as **text preprocessing**. It’s the essential first step for any NLP task.\n",
    "\n",
    "Without preprocessing, your computer would interpret `\"the\"`, `\"The\"`, and `\"<p>The\"` as completely different words. Preprocessing brings structure and consistency to raw text, ensuring that models can interpret it meaningfully.\n",
    "\n",
    "## Common Text Preprocessing Tasks\n",
    "\n",
    "* **Noise Removal**\n",
    "  Strip text of irrelevant or distracting elements such as HTML tags, punctuation, or special characters.\n",
    "  *Example:* removing `<p>`, `#`, `@`, etc.\n",
    "\n",
    "* **Tokenization**\n",
    "  Split text into smaller units (tokens) such as words or sentences.\n",
    "  *Example:* `\"NLP is fun!\" → [\"NLP\", \"is\", \"fun\"]`\n",
    "\n",
    "* **Normalization**\n",
    "  Bring all tokens to a standard format. This can include several sub-steps:\n",
    "\n",
    "  * **Stemming** — A rough cut that chops off prefixes and suffixes.\n",
    "    *Example:* “booing” → “boo”, “booed” → “boo”, but “computer” → “comput”.\n",
    "\n",
    "  * **Lemmatization** — A more refined approach using linguistic knowledge to find base forms.\n",
    "    *Example:* “am” → “be”, “are” → “be”.\n",
    "\n",
    "Other helpful operations:\n",
    "\n",
    "* Lowercasing text for consistency\n",
    "* Removing **stopwords** (e.g., “the”, “is”, “and”)\n",
    "* Fixing **spelling errors** or typos\n",
    "\n",
    "## Tools You’ll Use\n",
    "\n",
    "Most of these steps can be handled efficiently with **Regular Expressions (Regex)** and **NLTK (Natural Language Toolkit)** in Python.\n",
    "These tools make text cleaning systematic, repeatable, and ready for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e7cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import all necessary libraries for text preprocessing\n",
    "# - re: for regex-based text cleaning (removing punctuation and symbols)\n",
    "# - nltk: for tokenization and linguistic processing\n",
    "# - PorterStemmer: for stemming words (cutting down to their root form)\n",
    "# - WordNetLemmatizer: for lemmatizing words (smarter root form extraction)\n",
    "# - get_part_of_speech: helper function to map tokens to POS tags (noun, verb, etc.)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from utils.part_of_speech import get_part_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488b28ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['so', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'i', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'i', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "Lemmatized text:\n",
      "['So', 'many', 'squid', 'be', 'jump', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'pack', 'valise', 'I', 'go', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arrive', 'She', 'hardly', 'even', 'notice']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define the sample text to be processed\n",
    "# This text contains verbs, nouns, and adverbs that will demonstrate how\n",
    "# stemming and lemmatization treat words differently.\n",
    "text = (\n",
    "    \"So many squids are jumping out of suitcases these days that you can barely go anywhere \"\n",
    "    \"without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, \"\n",
    "    \"and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. \"\n",
    "    \"She hardly even noticed.\"\n",
    ")\n",
    "\n",
    "# Step 3: Clean the text using regex to remove punctuation and special characters\n",
    "# The pattern '\\W+' matches any non-alphanumeric character and replaces it with a space.\n",
    "cleaned = re.sub(r'\\W+', ' ', text)\n",
    "\n",
    "# Step 4: Tokenize the cleaned text into a list of individual words\n",
    "# This allows us to perform stemming and lemmatization on each token.\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "# Step 5: Initialize the PorterStemmer and apply it to all tokens\n",
    "# Stemming reduces words to their root forms but can sometimes produce non-dictionary results.\n",
    "# Example: \"jumping\" → \"jump\", \"hardly\" → \"hardli\"\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "\n",
    "# Step 6: Initialize the WordNetLemmatizer and apply it to all tokens\n",
    "# Lemmatization is context-aware when provided with part-of-speech tags.\n",
    "# It produces cleaner, dictionary-based root forms (e.g., \"went\" → \"go\").\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "\n",
    "# Step 7: Print out the results to compare stemming vs lemmatization\n",
    "# You’ll notice that stemming produces rougher cuts, while lemmatization keeps valid words.\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139f834",
   "metadata": {},
   "source": [
    "# Parsing Text\n",
    "\n",
    "After cleaning and preprocessing your text, the next step is to understand how words **relate to each other** — this is where **parsing** comes in. Parsing is the process of segmenting text and analyzing its **syntactic structure** (grammar and relationships between words).\n",
    "\n",
    "## Key Parsing Techniques in NLP\n",
    "\n",
    "* **Part-of-Speech (POS) Tagging**\n",
    "  POS tagging identifies whether each word is a **noun**, **verb**, **adjective**, or other grammatical category.\n",
    "  With NLTK, this is automatic — and usually faster (and more accurate) than your grammar teacher.\n",
    "\n",
    "* **Named Entity Recognition (NER)**\n",
    "  NER detects **proper nouns** like people, places, and organizations.\n",
    "  Example: “**Natalia** moved to **Berlin**” → entities = `[Natalia: PERSON, Berlin: LOCATION]`.\n",
    "  Recognizing named entities helps infer the **topic** or **context** of the text.\n",
    "\n",
    "* **Dependency Grammar Trees**\n",
    "  These structures show how words depend on one another — for example, which noun a verb acts upon.\n",
    "  Libraries like **spaCy** make this process much easier by automatically building dependency trees, although even they can struggle with ambiguous sentences.\n",
    "\n",
    "## Syntax Ambiguity Example\n",
    "\n",
    "Consider this sentence:\n",
    "\n",
    "> *I saw a cow under a tree with binoculars.*\n",
    "\n",
    "Who has the binoculars — you, the cow, or the tree?\n",
    "Even humans need context to interpret it correctly! This shows why **syntactic parsing** is both complex and essential.\n",
    "\n",
    "## Regex and Phrase Chunking\n",
    "\n",
    "For simpler, rule-based parsing, **Regular Expressions (Regex)** can be used to extract structured patterns like:\n",
    "\n",
    "* Email addresses\n",
    "* Postal codes\n",
    "* Specific phrase patterns (when combined with POS tagging)\n",
    "\n",
    "Regex parsing provides flexibility and control, especially for well-defined textual patterns, complementing statistical and deep learning–based parsing methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b751e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.8-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: jinja2 in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from spacy) (63.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.41.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-extensions>=4.14.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.3)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (6.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/alamanna1/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Downloading spacy-3.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m  \u001b[33m0:00:13\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (204 kB)\n",
      "Downloading murmurhash-1.0.13-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "Downloading preshed-3.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (795 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m795.1/795.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.8-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading blis-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.2-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: cymem, wasabi, typing-extensions, spacy-loggers, spacy-legacy, murmurhash, catalogue, blis, annotated-types, typing-inspection, typer-slim, srsly, pydantic-core, preshed, cloudpathlib, pydantic, confection, weasel, thinc, spacy\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.12.2\n",
      "\u001b[2K    Uninstalling typing_extensions-4.12.2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K      Successfully uninstalled typing_extensions-4.12.2━━━━━━━━━━━\u001b[0m \u001b[32m 2/20\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: pydantic0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/20\u001b[0m [srsly]-inspection]\n",
      "\u001b[2K    Found existing installation: pydantic 1.9.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/20\u001b[0m [srsly]\n",
      "\u001b[2K    Uninstalling pydantic-1.9.2:90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/20\u001b[0m [srsly]\n",
      "\u001b[2K      Successfully uninstalled pydantic-1.9.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/20\u001b[0m [srsly]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/20\u001b[0m [spacy]m19/20\u001b[0m [spacy]]c]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.11.1 requires pyarrow>=4.0, which is not installed.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow-cpu 2.10.0 requires keras<2.11,>=2.10.0, but you have keras 3.8.0 which is incompatible.\n",
      "tensorflow-cpu 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-cpu 2.10.0 requires tensorboard<2.11,>=2.10, but you have tensorboard 2.18.0 which is incompatible.\n",
      "tokenizers 0.14.0 requires huggingface_hub<0.17,>=0.16.4, but you have huggingface-hub 0.31.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.11 murmurhash-1.0.13 preshed-3.0.10 pydantic-2.12.4 pydantic-core-2.41.5 spacy-3.8.8 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.8 typer-slim-0.20.0 typing-extensions-4.15.0 typing-inspection-0.4.2 wasabi-1.1.3 weasel-0.4.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/alamanna1/.pyenv/versions/3.10.6/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/home/alamanna1/.pyenv/versions/3.10.6/lib/python3.10/runpy.py\", line 146, in _get_module_details\n",
      "    return _get_module_details(pkg_main_name, error)\n",
      "  File \"/home/alamanna1/.pyenv/versions/3.10.6/lib/python3.10/runpy.py\", line 110, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"/home/alamanna1/.pyenv/versions/lewagon/lib/python3.10/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/home/alamanna1/.pyenv/versions/lewagon/lib/python3.10/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/home/alamanna1/.pyenv/versions/lewagon/lib/python3.10/site-packages/spacy/compat.py\", line 5, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/home/alamanna1/.pyenv/versions/lewagon/lib/python3.10/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/home/alamanna1/.pyenv/versions/lewagon/lib/python3.10/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/home/alamanna1/.pyenv/versions/lewagon/lib/python3.10/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/home/alamanna1/.pyenv/versions/lewagon/lib/python3.10/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/home/alamanna1/.pyenv/versions/lewagon/lib/python3.10/site-packages/torch/__init__.py\", line 1382, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/alamanna1/.pyenv/versions/lewagon/lib/python3.10/site-packages/torch/functional.py\", line 7, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/alamanna1/.pyenv/versions/lewagon/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/alamanna1/.pyenv/versions/lewagon/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/alamanna1/.pyenv/versions/lewagon/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/alamanna1/.pyenv/versions/lewagon/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed5fb4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        jumping                \n",
      "  _________|________________    \n",
      " |   |   squids    out      |  \n",
      " |   |     |        |       |   \n",
      " |   |    many      of     days\n",
      " |   |     |        |       |   \n",
      "are  .     So   suitcases these\n",
      "\n",
      "          go                       \n",
      "  ________|____________________     \n",
      " |   |    |       |      |  without\n",
      " |   |    |       |      |     |    \n",
      " |   |    |       |      |   seeing\n",
      " |   |    |       |      |     |    \n",
      "You can barely anywhere  .    one  \n",
      "\n",
      "          went               \n",
      "  _________|_________         \n",
      " |   |     to        |       \n",
      " |   |     |         |        \n",
      " |   |  dentist     day      \n",
      " |   |     |      ___|____    \n",
      " I   .    the   the     other\n",
      "\n",
      "                   saw                           \n",
      "  __________________|_________                    \n",
      " |   |   |    |              jump                \n",
      " |   |   |    |      _________|__________         \n",
      " |   |   |    |     |    |    |         out      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |    |     |    |    |          of      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |    |     |    |    |         bag      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |   Sure   |    |    |       dentist    \n",
      " |   |   |    |     |    |    |     _____|_____   \n",
      " ,   I   .  enough  an angry one   my          's\n",
      "\n",
      "    noticed         \n",
      "  _____|__________   \n",
      "She  hardly even  . \n",
      "\n",
      "              jumps                    \n",
      "  ______________|________               \n",
      " |        |             over           \n",
      " |        |              |              \n",
      " |        |             dog            \n",
      " |        |           ___|_______       \n",
      " |       fox         |   |      with   \n",
      " |    ____|_____     |   |       |      \n",
      " .  The quick brown the lazy binoculars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import required libraries and the example squid text\n",
    "# - spaCy: for dependency parsing\n",
    "# - NLTK Tree: to visualize syntax trees\n",
    "# - squids_text: example text with silly squid sentences\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "squids_text = \"So many squids are jumping out of suitcases these days. You can barely go anywhere without seeing one. I went to the dentist the other day. Sure enough, I saw an angry one jump out of my dentist's bag. She hardly even noticed.\"\n",
    "\n",
    "# Step 2: Load the English dependency parser\n",
    "# Try multiple options for compatibility; prompt installation if missing.\n",
    "try:\n",
    "    dependency_parser = spacy.load(\"en\")\n",
    "except OSError:\n",
    "    try:\n",
    "        dependency_parser = spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        raise OSError(\"No English spaCy model found. Install it with: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "# Step 3: Parse the example squid text into a spaCy Doc object\n",
    "parsed_squids = dependency_parser(squids_text)\n",
    "\n",
    "# Step 4: Define a custom sentence and parse it as well\n",
    "# You can replace this text to visualize your own sentence structure.\n",
    "my_sentence = \"The quick brown fox jumps over the lazy dog with binoculars.\"\n",
    "my_parsed_sentence = dependency_parser(my_sentence)\n",
    "\n",
    "# Step 5: Define a recursive helper to convert a spaCy parse tree into an NLTK Tree\n",
    "# Each node is a token, and its children are syntactically related words.\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        parsed_child_nodes = [to_nltk_tree(child) for child in node.children]\n",
    "        return Tree(node.text, parsed_child_nodes)\n",
    "    else:\n",
    "        return node.text\n",
    "\n",
    "# Step 6: Print dependency trees for the squid sentences\n",
    "# Each tree shows how words depend on each other syntactically.\n",
    "for sent in parsed_squids.sents:\n",
    "    to_nltk_tree(sent.root).pretty_print()\n",
    "\n",
    "# Step 7: Print dependency tree for the custom sentence\n",
    "for sent in my_parsed_sentence.sents:\n",
    "    to_nltk_tree(sent.root).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a37db",
   "metadata": {},
   "source": [
    "# Language Models: Bag-of-Words\n",
    "\n",
    "Language models help computers understand and predict language by analyzing how often certain words, letters, or phrases appear in a collection of texts, known as a *corpus*. These models are **probabilistic**, meaning they estimate how likely a given word or phrase is to occur based on patterns found in data. Once trained, they can make predictions on new, unseen text.\n",
    "\n",
    "One of the simplest types is the **unigram** or **bag-of-words** model. It completely ignores grammar and word order, focusing only on how many times each word appears. For instance, after preprocessing and tokenizing the sentence:\n",
    "\n",
    "> “The squids jumped out of the suitcases.”\n",
    "\n",
    "the model would produce a mapping like:\n",
    "\n",
    "```python\n",
    "{\"the\": 2, \"squid\": 1, \"jump\": 1, \"out\": 1, \"of\": 1, \"suitcase\": 1}\n",
    "```\n",
    "\n",
    "If we analyze another sentence —\n",
    "\n",
    "> “Why are your suitcases full of jumping squids?”\n",
    "\n",
    "we’d get:\n",
    "\n",
    "```python\n",
    "{\"why\": 1, \"be\": 1, \"your\": 1, \"suitcase\": 1, \"full\": 1, \"of\": 1, \"jump\": 1, \"squid\": 1}\n",
    "```\n",
    "\n",
    "Even though the two sentences have different word orders and grammatical structures, the key words *“jump,” “squid,”* and *“suitcase”* appear in both.\n",
    "\n",
    "This illustrates why bag-of-words is useful for tasks like **topic modeling** or **sentiment analysis** — it captures the main subjects of a text without worrying about syntax. However, when word order or grammatical relationships matter, more advanced models are needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc658b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alamanna1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alamanna1/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alamanna1/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['however', 'egg', 'get', 'large', 'large', 'human', 'come', 'within', 'yard', 'saw', 'eye', 'nose', 'mouth', 'come', 'close', 'saw', 'clearly', 'humpty', 'dumpty', 'cant', 'anybody', 'else', 'say', 'im', 'certain', 'name', 'write', 'face', 'might', 'write', 'hundred', 'time', 'easily', 'enormous', 'face', 'humpty', 'dumpty', 'sit', 'leg', 'cross', 'like', 'turk', 'top', 'high', 'wallsuch', 'narrow', 'one', 'alice', 'quite', 'wonder', 'could', 'keep', 'balanceand', 'eye', 'steadily', 'fix', 'opposite', 'direction', 'didnt', 'take', 'least', 'notice', 'think', 'must', 'stuff', 'figure', 'exactly', 'like', 'egg', 'say', 'aloud', 'stand', 'hand', 'ready', 'catch', 'every', 'moment', 'expect', 'fall', 'provoke', 'humpty', 'dumpty', 'say', 'long', 'silence', 'look', 'away', 'alice', 'speak', 'call', 'eggvery', 'say', 'look', 'like', 'egg', 'sir', 'alice', 'gently', 'explain', 'egg', 'pretty', 'know', 'add', 'hop', 'turn', 'remark', 'sort', 'compliment', 'people', 'say', 'humpty', 'dumpty', 'look', 'away', 'usual', 'sense', 'baby', 'alice', 'didnt', 'know', 'say', 'wasnt', 'like', 'conversation', 'think', 'never', 'say', 'anything', 'fact', 'last', 'remark', 'evidently', 'address', 'treeso', 'stand', 'softly', 'repeat', 'humpty', 'dumpty', 'sit', 'wall', 'humpty', 'dumpty', 'great', 'fall', 'king', 'horse', 'king', 'men', 'couldnt', 'put', 'humpty', 'dumpty', 'place', 'last', 'line', 'much', 'long', 'poetry', 'add', 'almost', 'loud', 'forget', 'humpty', 'dumpty', 'would', 'hear', 'dont', 'stand', 'chatter', 'like', 'humpty', 'dumpty', 'say', 'look', 'first', 'time', 'tell', 'name', 'business', 'name', 'alice', 'stupid', 'enough', 'name', 'humpty', 'dumpty', 'interrupt', 'impatiently', 'mean', 'must', 'name', 'mean', 'something', 'alice', 'ask', 'doubtfully', 'course', 'must', 'humpty', 'dumpty', 'say', 'short', 'laugh', 'name', 'mean', 'shape', 'amand', 'good', 'handsome', 'shape', 'name', 'like', 'might', 'shape', 'almost', 'sit', 'alone', 'say', 'alice', 'wish', 'begin', 'argument', 'there', 'nobody', 'cry', 'humpty', 'dumpty', 'think', 'didnt', 'know', 'answer', 'ask', 'another', 'dont', 'think', 'youd', 'safe', 'grind', 'alice', 'go', 'idea', 'make', 'another', 'riddle', 'simply', 'good', 'natured', 'anxiety', 'queer', 'creature', 'wall', 'narrow', 'tremendously', 'easy', 'riddle', 'ask', 'humpty', 'dumpty', 'growl', 'course', 'dont', 'think', 'ever', 'fall', 'offwhich', 'there', 'chance', 'ofbut', 'purse', 'lip', 'look', 'solemn', 'grand', 'alice', 'could', 'hardly', 'help', 'laugh', 'fall', 'go', 'king', 'promise', 'mewith', 'mouthtoto', 'send', 'horse', 'men', 'alice', 'interrupt', 'rather', 'unwisely', 'declare', 'thats', 'bad', 'humpty', 'dumpty', 'cry', 'break', 'sudden', 'passion', 'youve', 'listen', 'doorsand', 'behind', 'treesand', 'chimneysor', 'couldnt', 'know', 'havent', 'indeed', 'alice', 'say', 'gently', 'book', 'ah', 'well', 'may', 'write', 'thing', 'book', 'humpty', 'dumpty', 'say', 'calm', 'tone', 'thats', 'call', 'history', 'england', 'take', 'good', 'look', 'im', 'one', 'speak', 'king', 'mayhap', 'youll', 'never', 'see', 'another', 'show', 'im', 'proud', 'may', 'shake', 'hand', 'grin', 'almost', 'ear', 'ear', 'lean', 'forward', 'nearly', 'possible', 'fell', 'wall', 'offer', 'alice', 'hand', 'watch', 'little', 'anxiously', 'take', 'smile', 'much', 'end', 'mouth', 'might', 'meet', 'behind', 'think', 'dont', 'know', 'would', 'happen', 'head', 'im', 'afraid', 'would', 'come', 'yes', 'horse', 'men', 'humpty', 'dumpty', 'go', 'theyd', 'pick', 'minute', 'would', 'however', 'conversation', 'go', 'little', 'fast', 'let', 'go', 'back', 'last', 'remark', 'one', 'im', 'afraid', 'cant', 'quite', 'remember', 'alice', 'say', 'politely', 'case', 'start', 'fresh', 'say', 'humpty', 'dumpty', 'turn', 'choose', 'subject', 'talk', 'game', 'think', 'alice', 'here', 'question', 'old', 'say', 'alice', 'make', 'short', 'calculation', 'say', 'seven', 'year', 'six', 'month', 'wrong', 'humpty', 'dumpty', 'exclaim', 'triumphantly', 'never', 'say', 'word', 'like', 'though', 'mean', 'old', 'alice', 'explain', 'id', 'mean', 'id', 'say', 'say', 'humpty', 'dumpty']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/alamanna1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 1: Import libraries ----------\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from utils.looking_glass import looking_glass_text\n",
    "from utils.part_of_speech import get_part_of_speech\n",
    "\n",
    "# Download required NLTK resources (only needed once)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# ---------- Step 2: Load text ----------\n",
    "text = looking_glass_text  # replace with your own string if needed\n",
    "\n",
    "# ---------- Step 3: Preprocess ----------\n",
    "# Lowercase & remove punctuation\n",
    "cleaned = re.sub(r\"\\W+\", \" \", text).lower()\n",
    "\n",
    "# Tokenize into words\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "# Remove stopwords (common words like \"the\", \"and\", etc.)\n",
    "stop_words = stopwords.words(\"english\")\n",
    "filtered = [word for word in tokenized if word not in stop_words]\n",
    "\n",
    "# ---------- Step 4: Lemmatize ----------\n",
    "normalizer = WordNetLemmatizer()\n",
    "normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in filtered]\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14e2e422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:\n",
      "Counter({'humpty': 19, 'dumpty': 19, 'say': 19, 'alice': 16, 'name': 7, 'like': 7, 'think': 7, 'look': 6, 'im': 5, 'know': 5, 'mean': 5, 'go': 5, 'egg': 4, 'fall': 4, 'king': 4, 'would': 4, 'dont': 4, 'come': 3, 'write': 3, 'might': 3, 'sit': 3, 'one': 3, 'didnt': 3, 'take': 3, 'must': 3, 'stand': 3, 'hand': 3, 'remark': 3, 'never': 3, 'last': 3, 'wall': 3, 'horse': 3, 'men': 3, 'almost': 3, 'ask': 3, 'shape': 3, 'good': 3, 'another': 3, 'however': 2, 'large': 2, 'saw': 2, 'eye': 2, 'mouth': 2, 'cant': 2, 'face': 2, 'time': 2, 'narrow': 2, 'quite': 2, 'could': 2, 'long': 2, 'away': 2, 'speak': 2, 'call': 2, 'gently': 2, 'explain': 2, 'add': 2, 'turn': 2, 'conversation': 2, 'couldnt': 2, 'much': 2, 'interrupt': 2, 'course': 2, 'short': 2, 'laugh': 2, 'there': 2, 'cry': 2, 'make': 2, 'riddle': 2, 'thats': 2, 'behind': 2, 'book': 2, 'may': 2, 'ear': 2, 'little': 2, 'afraid': 2, 'old': 2, 'id': 2, 'get': 1, 'human': 1, 'within': 1, 'yard': 1, 'nose': 1, 'close': 1, 'clearly': 1, 'anybody': 1, 'else': 1, 'certain': 1, 'hundred': 1, 'easily': 1, 'enormous': 1, 'leg': 1, 'cross': 1, 'turk': 1, 'top': 1, 'high': 1, 'wallsuch': 1, 'wonder': 1, 'keep': 1, 'balanceand': 1, 'steadily': 1, 'fix': 1, 'opposite': 1, 'direction': 1, 'least': 1, 'notice': 1, 'stuff': 1, 'figure': 1, 'exactly': 1, 'aloud': 1, 'ready': 1, 'catch': 1, 'every': 1, 'moment': 1, 'expect': 1, 'provoke': 1, 'silence': 1, 'eggvery': 1, 'sir': 1, 'pretty': 1, 'hop': 1, 'sort': 1, 'compliment': 1, 'people': 1, 'usual': 1, 'sense': 1, 'baby': 1, 'wasnt': 1, 'anything': 1, 'fact': 1, 'evidently': 1, 'address': 1, 'treeso': 1, 'softly': 1, 'repeat': 1, 'great': 1, 'put': 1, 'place': 1, 'line': 1, 'poetry': 1, 'loud': 1, 'forget': 1, 'hear': 1, 'chatter': 1, 'first': 1, 'tell': 1, 'business': 1, 'stupid': 1, 'enough': 1, 'impatiently': 1, 'something': 1, 'doubtfully': 1, 'amand': 1, 'handsome': 1, 'alone': 1, 'wish': 1, 'begin': 1, 'argument': 1, 'nobody': 1, 'answer': 1, 'youd': 1, 'safe': 1, 'grind': 1, 'idea': 1, 'simply': 1, 'natured': 1, 'anxiety': 1, 'queer': 1, 'creature': 1, 'tremendously': 1, 'easy': 1, 'growl': 1, 'ever': 1, 'offwhich': 1, 'chance': 1, 'ofbut': 1, 'purse': 1, 'lip': 1, 'solemn': 1, 'grand': 1, 'hardly': 1, 'help': 1, 'promise': 1, 'mewith': 1, 'mouthtoto': 1, 'send': 1, 'rather': 1, 'unwisely': 1, 'declare': 1, 'bad': 1, 'break': 1, 'sudden': 1, 'passion': 1, 'youve': 1, 'listen': 1, 'doorsand': 1, 'treesand': 1, 'chimneysor': 1, 'havent': 1, 'indeed': 1, 'ah': 1, 'well': 1, 'thing': 1, 'calm': 1, 'tone': 1, 'history': 1, 'england': 1, 'mayhap': 1, 'youll': 1, 'see': 1, 'show': 1, 'proud': 1, 'shake': 1, 'grin': 1, 'lean': 1, 'forward': 1, 'nearly': 1, 'possible': 1, 'fell': 1, 'offer': 1, 'watch': 1, 'anxiously': 1, 'smile': 1, 'end': 1, 'meet': 1, 'happen': 1, 'head': 1, 'yes': 1, 'theyd': 1, 'pick': 1, 'minute': 1, 'fast': 1, 'let': 1, 'back': 1, 'remember': 1, 'politely': 1, 'case': 1, 'start': 1, 'fresh': 1, 'choose': 1, 'subject': 1, 'talk': 1, 'game': 1, 'here': 1, 'question': 1, 'calculation': 1, 'seven': 1, 'year': 1, 'six': 1, 'month': 1, 'wrong': 1, 'exclaim': 1, 'triumphantly': 1, 'word': 1, 'though': 1})\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 5: Create Bag-of-Words ----------\n",
    "# Each unique word gets counted — order doesn’t matter\n",
    "bag_of_looking_glass_words = Counter(normalized)\n",
    "\n",
    "# ---------- Step 6: Display most common words ----------\n",
    "print(\"Bag of words:\")\n",
    "print(bag_of_looking_glass_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3f58108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:\n",
      "Counter({'artificial': 1, 'intelligence': 1, 'transform': 1, 'industry': 1, 'create': 1, 'new': 1, 'opportunity': 1, 'every': 1, 'day': 1})\n"
     ]
    }
   ],
   "source": [
    "# ---------- Do it with a new sentence ----------\n",
    "\n",
    "# ---------- Step 2: Load text ----------\n",
    "text = \"Artificial intelligence is transforming industries and creating new opportunities every day.\"\n",
    "\n",
    "# ---------- Step 3: Preprocess ----------\n",
    "# Lowercase & remove punctuation\n",
    "cleaned = re.sub(r\"\\W+\", \" \", text).lower()\n",
    "\n",
    "# Tokenize into words\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "# Remove stopwords (common words like \"the\", \"and\", etc.)\n",
    "stop_words = stopwords.words(\"english\")\n",
    "filtered = [word for word in tokenized if word not in stop_words]\n",
    "\n",
    "# ---------- Step 4: Lemmatize ----------\n",
    "normalizer = WordNetLemmatizer()\n",
    "normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in filtered]\n",
    "\n",
    "# ---------- Step 5: Create Bag-of-Words ----------\n",
    "# Each unique word gets counted — order doesn’t matter\n",
    "bag_of_looking_glass_words = Counter(normalized)\n",
    "\n",
    "# ---------- Step 6: Display most common words ----------\n",
    "print(\"Bag of words:\")\n",
    "print(bag_of_looking_glass_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90643340",
   "metadata": {},
   "source": [
    "# **Language Models: N-Gram and Neural Language Models (NLM)**\n",
    "\n",
    "When we want to analyze entire phrases or predict the next word in a sentence, we need a model that looks beyond individual words.\n",
    "Unlike **Bag-of-Words**, which ignores word order, the **N-Gram** model takes into account how words appear in sequence. It examines a continuous window of *n* words and calculates how likely each word is to appear given the preceding ones.\n",
    "\n",
    "For instance, in a **bigram** model (*n = 2*), the sentence:\n",
    "\n",
    "> “The squids jumped out of the suitcases. The squids were furious.”\n",
    "\n",
    "would produce frequency counts such as:\n",
    "\n",
    "```python\n",
    "{('', 'the'): 2, ('the', 'squids'): 2, ('squids', 'jumped'): 1,\n",
    " ('jumped', 'out'): 1, ('out', 'of'): 1, ('of', 'the'): 1,\n",
    " ('the', 'suitcases'): 1, ('suitcases', ''): 1,\n",
    " ('squids', 'were'): 1, ('were', 'furious'): 1, ('furious', ''): 1}\n",
    "```\n",
    "\n",
    "This approach captures short-range dependencies — it “remembers” what word tends to follow another — making it better suited for predicting the next word or analyzing common word pairs.\n",
    "\n",
    "However, the N-Gram model faces two main challenges:\n",
    "\n",
    "1. **Unknown words:** if the model encounters a new term like *“mailbox”* that never appeared during training, it can’t assign a meaningful probability. Techniques such as *smoothing* can partially fix this but not perfectly.\n",
    "2. **Data sparsity:** as *n* grows (for example, from bigram to trigram or higher), the number of unique sequences explodes, while examples for each become rare — making reliable probability estimates difficult.\n",
    "\n",
    "To overcome these limitations, researchers developed **Neural Language Models (NLMs)**.\n",
    "Instead of relying on explicit counts, NLMs learn to predict words based on their learned **contextual representations** using neural networks. This enables them to generalize to unseen words or sequences and capture long-range dependencies more naturally.\n",
    "\n",
    "Common NLM architectures include **LSTMs (Long Short-Term Memory networks)** and **Transformer models**, which form the foundation for today’s most advanced language systems like GPT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "701588e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------- N-Gram Exploration with 'Through the Looking Glass' Text ----------\n",
    "\n",
    "import nltk, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from utils.looking_glass_full import looking_glass_full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bf69c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking Glass Bigrams:\n",
      "[(('of', 'the'), 101), (('said', 'the'), 98), (('in', 'a'), 97), (('in', 'the'), 90), (('as', 'she'), 82), (('you', 'know'), 72), (('a', 'little'), 68), (('the', 'queen'), 67), (('said', 'alice'), 67), (('to', 'the'), 66)]\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 1: Preprocess the original text ----------\n",
    "cleaned = re.sub(r'\\W+', ' ', looking_glass_full_text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "# ---------- Step 2: Create different N-Grams ----------\n",
    "# Bigrams (n = 2)\n",
    "looking_glass_bigrams = ngrams(tokenized, 2)\n",
    "looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)\n",
    "\n",
    "print(\"Looking Glass Bigrams:\")\n",
    "print(looking_glass_bigrams_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccb077e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking Glass Trigrams:\n",
      "[(('the', 'red', 'queen'), 54), (('the', 'white', 'queen'), 31), (('said', 'in', 'a'), 21), (('she', 'went', 'on'), 18), (('said', 'the', 'red'), 17), (('thought', 'to', 'herself'), 16), (('the', 'queen', 'said'), 16), (('said', 'to', 'herself'), 14), (('said', 'humpty', 'dumpty'), 14), (('the', 'knight', 'said'), 14)]\n"
     ]
    }
   ],
   "source": [
    "# Trigrams (n = 3)\n",
    "looking_glass_trigrams = ngrams(tokenized, 3)\n",
    "looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)\n",
    "\n",
    "print(\"\\nLooking Glass Trigrams:\")\n",
    "print(looking_glass_trigrams_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e4f0d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking Glass 5-grams:\n",
      "[(('one', 'and', 'one', 'and', 'one'), 8), (('and', 'one', 'and', 'one', 'and'), 7), (('for', 'a', 'minute', 'or', 'two'), 6), (('the', 'lion', 'and', 'the', 'unicorn'), 6), (('as', 'well', 'as', 'she', 'could'), 5), (('is', 'worth', 'a', 'thousand', 'pounds'), 4), (('the', 'walrus', 'and', 'the', 'carpenter'), 4), (('said', 'to', 'herself', 'as', 'she'), 4), (('twas', 'brillig', 'and', 'the', 'slithy'), 3), (('brillig', 'and', 'the', 'slithy', 'toves'), 3)]\n"
     ]
    }
   ],
   "source": [
    "# Larger N-Grams (n > 3)\n",
    "looking_glass_ngrams = ngrams(tokenized, 5)\n",
    "looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)\n",
    "\n",
    "print(\"\\nLooking Glass 5-grams:\")\n",
    "print(looking_glass_ngrams_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7826ea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking Glass 20-grams:\n",
      "[(('twas', 'brillig', 'and', 'the', 'slithy', 'toves', 'did', 'gyre', 'and', 'gimble', 'in', 'the', 'wabe', 'all', 'mimsy', 'were', 'the', 'borogoves', 'and', 'the'), 3), (('brillig', 'and', 'the', 'slithy', 'toves', 'did', 'gyre', 'and', 'gimble', 'in', 'the', 'wabe', 'all', 'mimsy', 'were', 'the', 'borogoves', 'and', 'the', 'mome'), 3), (('and', 'the', 'slithy', 'toves', 'did', 'gyre', 'and', 'gimble', 'in', 'the', 'wabe', 'all', 'mimsy', 'were', 'the', 'borogoves', 'and', 'the', 'mome', 'raths'), 3), (('the', 'slithy', 'toves', 'did', 'gyre', 'and', 'gimble', 'in', 'the', 'wabe', 'all', 'mimsy', 'were', 'the', 'borogoves', 'and', 'the', 'mome', 'raths', 'outgrabe'), 3), (('chapter', 'i', 'looking', 'glass', 'house', 'one', 'thing', 'was', 'certain', 'that', 'the', 'white', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it'), 1), (('i', 'looking', 'glass', 'house', 'one', 'thing', 'was', 'certain', 'that', 'the', 'white', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', 'it'), 1), (('looking', 'glass', 'house', 'one', 'thing', 'was', 'certain', 'that', 'the', 'white', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', 'it', 'was'), 1), (('glass', 'house', 'one', 'thing', 'was', 'certain', 'that', 'the', 'white', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', 'it', 'was', 'the'), 1), (('house', 'one', 'thing', 'was', 'certain', 'that', 'the', 'white', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', 'it', 'was', 'the', 'black'), 1), (('one', 'thing', 'was', 'certain', 'that', 'the', 'white', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', 'it', 'was', 'the', 'black', 'kittens'), 1)]\n"
     ]
    }
   ],
   "source": [
    "# Larger N-Grams (n > 3)\n",
    "looking_glass_20grams = ngrams(tokenized, 20)\n",
    "looking_glass_ngrams_frequency = Counter(looking_glass_20grams)\n",
    "\n",
    "print(\"\\nLooking Glass 20-grams:\")\n",
    "print(looking_glass_ngrams_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826ef33",
   "metadata": {},
   "source": [
    "# **Topic Models**\n",
    "\n",
    "Sometimes, a text is so long or complex that the main themes aren’t immediately clear. **Topic modeling** is a field of NLP designed to uncover these *hidden* or *latent* topics automatically.\n",
    "It groups together words that frequently appear in the same contexts, helping us identify the underlying structure or themes in a collection of documents.\n",
    "\n",
    "A common first step in this process is to calculate **term frequency–inverse document frequency (TF-IDF)**.\n",
    "While it might sound counterintuitive, TF-IDF actually *downweights* very common words (like “the” or “is”) and *upweights* rarer, more distinctive ones — because those unique words are often what define a topic.\n",
    "Python libraries such as **gensim** and **scikit-learn (sklearn)** provide easy-to-use tools for performing this weighting automatically.\n",
    "\n",
    "Once you have a TF-IDF representation, you can apply a statistical method called **Latent Dirichlet Allocation (LDA)**.\n",
    "LDA looks for words that tend to occur together across different documents, revealing clusters that represent potential topics — for instance, “suitcase,” “jump,” and “squid” might frequently appear in the same context, suggesting a “traveling squids” topic.\n",
    "\n",
    "To visualize these relationships, you can use **word2vec**, a neural embedding model that maps words into a spatial representation.\n",
    "In this space, words that share similar meanings or appear in similar contexts are placed closer together.\n",
    "So in our earlier squid example, *“suitcase,” “jump,”* and *“squid”* would likely form a tight cluster — a clear illustration of how words connect within topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9a8ed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Topics found by bag of words LDA ~~~\n",
      "Topic #1: hand eye son still leave\n",
      "Topic #2: hand mccarthy street find give\n",
      "Topic #3: young never walk word find\n",
      "Topic #4: street room house hand make\n",
      "Topic #5: majesty king sherlock photograph mr\n",
      "Topic #6: father mr case remark coroner\n",
      "Topic #7: mccarthy father son find young\n",
      "Topic #8: photograph room make find give\n",
      "Topic #9: leave father mr case back\n",
      "Topic #10: two speak case doubt side\n",
      "\n",
      "\n",
      "~~~ Topics found by tf-idf LDA ~~~\n",
      "Topic #1: circumstance thoroughly however truly miss\n",
      "Topic #2: scarlet many mumble advantage minor\n",
      "Topic #3: eight wave tie cloud double\n",
      "Topic #4: another consider still instinct many\n",
      "Topic #5: convey utter scene opening farm\n",
      "Topic #6: king say holmes call alarm\n",
      "Topic #7: hard favour effect write toss\n",
      "Topic #8: presumably watch understand three wire\n",
      "Topic #9: hold clergyman indicate six deadly\n",
      "Topic #10: holmes say upon know man\n"
     ]
    }
   ],
   "source": [
    "# ---------- Topic Modeling: BoW vs TF-IDF with custom stop_list ----------\n",
    "\n",
    "import nltk, re\n",
    "from utils.sherlock_holmes import (\n",
    "    bohemia_ch1, bohemia_ch2, bohemia_ch3,\n",
    "    boscombe_ch1, boscombe_ch2, boscombe_ch3\n",
    ")\n",
    "from utils.preprocessing import preprocess_text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# ---------- Prepare corpus ----------\n",
    "corpus = [bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3]\n",
    "preprocessed_corpus = [preprocess_text(ch) for ch in corpus]\n",
    "\n",
    "# ---------- Update stop_list (ensure >= 10 and deduplicated) ----------\n",
    "stop_list = [\n",
    "    \"say\", \"see\", \"holmes\", \"shall\", \"man\", \"upon\", \"know\", \"quite\", \"one\",\n",
    "    \"well\", \"could\", \"would\", \"take\", \"may\", \"think\", \"come\", \"go\", \"little\",\n",
    "    \"must\", \"look\"\n",
    "]\n",
    "stop_list = sorted(set(w.lower() for w in stop_list))  # dedupe & lowercase\n",
    "\n",
    "# ---------- Filter stop words from the preprocessed corpus for BoW ----------\n",
    "def filter_out_stop_words(corpus, stops):\n",
    "    out = []\n",
    "    for chapter in corpus:\n",
    "        tokens = chapter.split()\n",
    "        out.append(\" \".join([w for w in tokens if w not in stops]))\n",
    "    return out\n",
    "\n",
    "filtered_for_stops = filter_out_stop_words(preprocessed_corpus, stop_list)\n",
    "\n",
    "# ---------- Vectorize ----------\n",
    "bag_of_words_creator = CountVectorizer()\n",
    "bag_of_words = bag_of_words_creator.fit_transform(filtered_for_stops)\n",
    "\n",
    "tfidf_creator = TfidfVectorizer(min_df=0.2)\n",
    "tfidf = tfidf_creator.fit_transform(preprocessed_corpus)\n",
    "\n",
    "# ---------- LDA on BoW ----------\n",
    "lda_bow = LatentDirichletAllocation(\n",
    "    n_components=10, learning_method=\"online\", random_state=42\n",
    ")\n",
    "lda_bow_topics = lda_bow.fit_transform(bag_of_words)\n",
    "\n",
    "# ---------- LDA on TF-IDF ----------\n",
    "lda_tfidf = LatentDirichletAllocation(\n",
    "    n_components=10, learning_method=\"online\", random_state=42\n",
    ")\n",
    "lda_tfidf_topics = lda_tfidf.fit_transform(tfidf)\n",
    "\n",
    "# ---------- Helper to print top terms ----------\n",
    "def top_terms(model, vectorizer, top_k=5):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for topic_id, topic in enumerate(model.components_):\n",
    "        top_idxs = topic.argsort()[:-top_k-1:-1]\n",
    "        terms = \" \".join(feature_names[i] for i in top_idxs)\n",
    "        print(f\"Topic #{topic_id + 1}: {terms}\")\n",
    "\n",
    "print(\"~~~ Topics found by bag of words LDA ~~~\")\n",
    "top_terms(lda_bow, bag_of_words_creator, top_k=5)\n",
    "\n",
    "print(\"\\n\\n~~~ Topics found by tf-idf LDA ~~~\")\n",
    "top_terms(lda_tfidf, tfidf_creator, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1918a",
   "metadata": {},
   "source": [
    "# **Text Similarity**\n",
    "\n",
    "When you’ve ever had your phone autocorrect a message into something embarrassing, you’ve experienced one of the toughest challenges in Natural Language Processing: **text similarity**.\n",
    "Determining how closely two words or sentences resemble each other — whether in spelling, sound, or meaning — underlies many NLP tasks, from autocorrect and plagiarism detection to recommendation systems.\n",
    "\n",
    "A foundational approach to comparing words is the **Levenshtein distance** (also called *edit distance*).\n",
    "It measures the minimum number of changes — **insertions**, **deletions**, or **substitutions** — required to transform one word into another.\n",
    "For example, converting *“bees”* into *“beans”* takes one substitution (changing “e” → “a”) and one insertion (“n”), giving a distance of **2**.\n",
    "The smaller the distance, the more similar the words.\n",
    "\n",
    "Beyond spelling, similarity can also be **phonetic** — based on how words *sound*.\n",
    "Humans easily distinguish between *“euthanasia”* and *“youth in Asia”*, but for machines, this is extremely difficult.\n",
    "Advanced systems handle this by incorporating phonetic distance measures and even keyboard layout proximity to simulate human-like correction.\n",
    "\n",
    "Text similarity is also crucial in other contexts:\n",
    "\n",
    "* **Lexical similarity** compares the overlap in vocabulary (useful for plagiarism detection).\n",
    "* **Semantic similarity** measures shared meaning — for example, finding articles or books similar in theme or topic to one you’ve read.\n",
    "\n",
    "In essence, text similarity helps machines understand not just how words *look* or *sound*, but what they *mean* in relation to one another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36d31247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Levenshtein distance from 'fart' to 'target' is 3!\n",
      "The Levenshtein distance from 'code' to 'molds' is 3!\n",
      "The Levenshtein distance from 'chunk' to 'churn' is 2!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# NLTK has a built-in function\n",
    "# to check Levenshtein distance:\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "def print_levenshtein(string1, string2):\n",
    "    print(\"The Levenshtein distance from '{0}' to '{1}' is {2}!\".format(\n",
    "        string1, string2, edit_distance(string1, string2))\n",
    "    )\n",
    "\n",
    "# Check the distance between any two words here!\n",
    "print_levenshtein(\"fart\", \"target\")\n",
    "\n",
    "# Assign passing strings here:\n",
    "# \"code\" → \"cider\" needs 3 edits: substitute 'o'→'i', add 'r', substitute 'e'→'r'\n",
    "three_away_from_code = \"molds\"   # actually 3 edits from \"code\"\n",
    "\n",
    "# \"chunk\" → \"churn\" needs 2 edits: 'n'→'r', 'k'→'n'\n",
    "two_away_from_chunk = \"churn\"\n",
    "\n",
    "print_levenshtein(\"code\", three_away_from_code)\n",
    "print_levenshtein(\"chunk\", two_away_from_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ec8be",
   "metadata": {},
   "source": [
    "# Language Prediction & Text Generation\n",
    "\n",
    "How does your phone’s keyboard know what you want to type next? How does a search engine complete your query before you finish typing?\n",
    "These are all examples of **language prediction**, a core application of **Natural Language Processing (NLP)** that involves predicting text based on preceding words.\n",
    "\n",
    "## Common Applications\n",
    "\n",
    "* **Autosuggest** and **Autocomplete** (e.g., search bars, messaging apps)\n",
    "* **Suggested Replies** (e.g., “Sounds good!”, “See you soon!”)\n",
    "* **Predictive Text** in keyboards and chatbots\n",
    "\n",
    "## Choosing a Language Model\n",
    "\n",
    "Language prediction starts with selecting a **language model**, which defines how text sequences are represented and predicted.\n",
    "\n",
    "### 1. Bag of Words (BoW)\n",
    "\n",
    "While simple, **BoW** ignores word order. As a result, predictions often fall back to the most common words in the training corpus — not ideal for generating meaningful sequences.\n",
    "\n",
    "### 2. N-gram Models & Markov Chains\n",
    "\n",
    "An **n-gram** model uses a sliding window of *n* words (or characters) to predict the next token.\n",
    "It typically relies on a **Markov chain**, which is **memory-less** — predictions depend only on the current n-gram.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> Starting with “I ate so many grilled cheese”,\n",
    "> a **trigram** model (n=3) might predict **“sandwiches”**\n",
    "> because in the training corpus, the sequence “grilled cheese sandwiches” frequently follows “grilled cheese”.\n",
    "\n",
    "The Markov property limits this model’s understanding — it can’t “remember” context beyond its window.\n",
    "\n",
    "### 3. Neural Language Models (LSTMs)\n",
    "\n",
    "A more advanced method uses **Long Short-Term Memory (LSTM)** networks — a type of **recurrent neural network (RNN)**.\n",
    "LSTMs maintain an internal memory state, allowing them to model **long-term dependencies** in text.\n",
    "\n",
    "This makes them far more effective at generating coherent, contextually relevant text — a foundation for modern **text generation**, **translation**, and **chatbots**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52447ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there s buying a stairway lies on the truth by the present our shadows taller than our life is the opinion of chance my unconquerable soul there s still turns to pole i have the best revenge is humming and the stores are two meanings in case you will find\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries and training documents\n",
    "# - nltk: for tokenization\n",
    "# - re: for text cleaning\n",
    "# - random: for stochastic text generation\n",
    "# - defaultdict, deque: for managing key-value lists and context windows\n",
    "# - document1, document2, document3: contain your chosen text data (stories or lyrics)\n",
    "import nltk, re, random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, deque\n",
    "from utils.document1 import training_doc1\n",
    "from utils.document2 import training_doc2\n",
    "from utils.document3 import training_doc3\n",
    "\n",
    "# Step 2: Define a MarkovChain class to build and use a bigram-based model\n",
    "class MarkovChain:\n",
    "    def __init__(self):\n",
    "        # lookup_dict stores possible next words for each word in the corpus\n",
    "        self.lookup_dict = defaultdict(list)\n",
    "        self._seeded = False\n",
    "        self.__seed_me()\n",
    "\n",
    "    # Step 3: Seed the random number generator for reproducibility\n",
    "    def __seed_me(self, rand_seed=None):\n",
    "        if self._seeded is not True:\n",
    "            try:\n",
    "                if rand_seed is not None:\n",
    "                    random.seed(rand_seed)\n",
    "                else:\n",
    "                    random.seed()\n",
    "                self._seeded = True\n",
    "            except NotImplementedError:\n",
    "                self._seeded = False\n",
    "\n",
    "    # Step 4: Add a document’s text to the Markov chain\n",
    "    # This preprocesses the text and updates lookup_dict with word pairs\n",
    "    def add_document(self, text):\n",
    "        preprocessed_list = self._preprocess(text)\n",
    "        pairs = self.__generate_tuple_keys(preprocessed_list)\n",
    "        for pair in pairs:\n",
    "            self.lookup_dict[pair[0]].append(pair[1])\n",
    "\n",
    "    # Step 5: Preprocess text by cleaning and tokenizing\n",
    "    # - Removes non-alphanumeric characters\n",
    "    # - Converts to lowercase\n",
    "    # - Tokenizes using NLTK\n",
    "    def _preprocess(self, text):\n",
    "        cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "        tokenized = word_tokenize(cleaned)\n",
    "        return tokenized\n",
    "\n",
    "    # Step 6: Create word pairs (bigrams) for building the Markov chain\n",
    "    def __generate_tuple_keys(self, data):\n",
    "        if len(data) < 1:\n",
    "            return\n",
    "        for i in range(len(data) - 1):\n",
    "            yield [data[i], data[i + 1]]\n",
    "\n",
    "    # Step 7: Generate text based on the trained Markov chain\n",
    "    # - Starts from a random word\n",
    "    # - Predicts the next word based on probabilities from lookup_dict\n",
    "    def generate_text(self, max_length=50):\n",
    "        context = deque()\n",
    "        output = []\n",
    "        if len(self.lookup_dict) > 0:\n",
    "            self.__seed_me(rand_seed=len(self.lookup_dict))\n",
    "            chain_head = [list(self.lookup_dict)[0]]\n",
    "            context.extend(chain_head)\n",
    "\n",
    "            while len(output) < (max_length - 1):\n",
    "                next_choices = self.lookup_dict[context[-1]]\n",
    "                if len(next_choices) > 0:\n",
    "                    next_word = random.choice(next_choices)\n",
    "                    context.append(next_word)\n",
    "                    output.append(context.popleft())\n",
    "                else:\n",
    "                    break\n",
    "            output.extend(list(context))\n",
    "        return \" \".join(output)\n",
    "\n",
    "# Step 8: Initialize the MarkovChain and add the three training documents\n",
    "my_markov = MarkovChain()\n",
    "my_markov.add_document(training_doc1)\n",
    "my_markov.add_document(training_doc2)\n",
    "my_markov.add_document(training_doc3)\n",
    "\n",
    "# Step 9: Generate a short piece of text using the trained model\n",
    "generated_text = my_markov.generate_text()\n",
    "\n",
    "# Step 10: Print the generated text for inspection\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd7043",
   "metadata": {},
   "source": [
    "# Advanced NLP Topics\n",
    "\n",
    "You’ve now explored the foundations of **Natural Language Processing (NLP)** — but this is just the beginning. The field is vast, with advanced topics and applications driven largely by **deep learning** and **neural networks**.\n",
    "\n",
    "## Naive Bayes Classifiers\n",
    "\n",
    "A **Naive Bayes classifier** is a **supervised machine learning algorithm** that applies **Bayes’ theorem** to predict outcomes based on probabilities.\n",
    "It’s particularly effective for:\n",
    "\n",
    "* **Sentiment analysis** — determining whether text conveys positive or negative emotion\n",
    "* **Spam detection** — classifying emails as spam or legitimate messages\n",
    "\n",
    "Despite its simplicity, Naive Bayes remains a strong baseline model in NLP tasks.\n",
    "\n",
    "## Machine Translation\n",
    "\n",
    "Modern **machine translation** systems (like Google Translate) have improved dramatically using **neural networks** and **LSTM (Long Short-Term Memory)** architectures.\n",
    "However, even these models struggle with nuances such as idioms, cultural context, and tone — areas where human understanding still outperforms machines.\n",
    "\n",
    "## Accessibility and Assistive Technology\n",
    "\n",
    "NLP plays a major role in **language accessibility**:\n",
    "\n",
    "* **Text-to-speech (TTS)** converts written text into spoken words.\n",
    "* **Speech recognition** enables voice commands and dictation.\n",
    "\n",
    "These technologies, powered by neural language models, have made digital environments much more inclusive for people with disabilities.\n",
    "\n",
    "## Detecting Bias in Language\n",
    "\n",
    "NLP can also help uncover **bias** in text — whether political, social, or cultural.\n",
    "By analyzing patterns in word usage, tone, and framing, NLP tools can highlight subtle biases in speeches, articles, or media coverage, promoting more transparent communication.\n",
    "\n",
    "In short, NLP is evolving rapidly — from sentiment analysis to speech synthesis — transforming the way humans and machines understand each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a719bc79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, sys, importlib, pprint\n",
    "\n",
    "# Ensure project root is on sys.path\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# Hard-reload the modules to defeat caching\n",
    "import utils.review as rv\n",
    "import utils.train_matrix as tm\n",
    "importlib.reload(rv)\n",
    "importlib.reload(tm)\n",
    "\n",
    "# Now import the names exactly\n",
    "from utils.review import counter\n",
    "from utils.train_matrix import training_counts\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6e7c5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for your positive review!\n",
      "\n",
      "According to our trained Naive Bayes classifier, the probability that your review was negative was 28.0% and the probability it was positive was 72.0%.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Add your review as a string to be classified\n",
    "# Write a short text summarizing your experience or impression of the NLP lesson.\n",
    "review = (\n",
    "    \"The lesson on Natural Language Processing has been very engaging and informative, \"\n",
    "    \"introducing core NLP concepts clearly, though it contained so many new ideas that I \"\n",
    "    \"might need a bit more time to consolidate them.\"\n",
    ")\n",
    "\n",
    "# Step 3: Convert your review text into numerical feature counts\n",
    "# This uses the same vocabulary learned during training.\n",
    "review_counts = counter.transform([review])\n",
    "\n",
    "# Step 4: Initialize and train a Naive Bayes classifier on the provided dataset\n",
    "# The labels indicate 0 = negative and 1 = positive reviews.\n",
    "classifier = MultinomialNB()\n",
    "training_labels = [0] * 1000 + [1] * 1000\n",
    "classifier.fit(training_counts, training_labels)\n",
    "\n",
    "# Step 5: Predict the probability of the review being positive or negative\n",
    "neg = (classifier.predict_proba(review_counts)[0][0] * 100).round()\n",
    "pos = (classifier.predict_proba(review_counts)[0][1] * 100).round()\n",
    "\n",
    "# Step 6: Display a custom message depending on classification confidence\n",
    "if pos > 50:\n",
    "    print(\"Thank you for your positive review!\")\n",
    "elif neg > 50:\n",
    "    print(\"We're sorry this hasn't been the best possible lesson for you! We're always looking to improve.\")\n",
    "else:\n",
    "    print(\"Naive Bayes cannot determine if this is negative or positive. Thank you or we're sorry?\")\n",
    "\n",
    "# Step 7: Print the computed probabilities for transparency\n",
    "print(\n",
    "    \"\\nAccording to our trained Naive Bayes classifier, \"\n",
    "    \"the probability that your review was negative was {0}% \"\n",
    "    \"and the probability it was positive was {1}%.\".format(neg, pos)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298f619e",
   "metadata": {},
   "source": [
    "# Challenges and Considerations\n",
    "\n",
    "With the power of **Natural Language Processing (NLP)** comes significant responsibility. As NLP technologies continue to evolve, it’s important to reflect on the **ethical, linguistic, and privacy challenges** they bring.\n",
    "\n",
    "## 1. Language and Cultural Bias\n",
    "\n",
    "Many NLP tools are developed primarily for **English** and often by **English-speaking researchers**.\n",
    "As a result:\n",
    "\n",
    "* Models may perform poorly on other languages.\n",
    "* They can inherit **cultural and linguistic biases** specific to English speakers.\n",
    "* This creates inequities in global accessibility and accuracy.\n",
    "\n",
    "## 2. Sociolinguistic Diversity\n",
    "\n",
    "Even within English, variation is immense — by **region**, **social background**, **gender**, and **dialect**.\n",
    "Imagine if Amazon Alexa only understood **wealthy men from coastal U.S. cities** — this highlights how limited datasets can result in biased systems.\n",
    "When designing NLP systems, we must ask: *Who is the tool really built for?*\n",
    "\n",
    "## 3. Bias Propagation\n",
    "\n",
    "NLP systems can **amplify bias** present in their training data.\n",
    "Even with good intentions, developers may inadvertently create tools that reproduce or worsen societal prejudices.\n",
    "A key responsibility of NLP practitioners is to identify, mitigate, and document potential biases in both the data and the model behavior.\n",
    "\n",
    "## 4. Privacy Concerns\n",
    "\n",
    "NLP tools often depend on **large volumes of personal data** — conversations, emails, or voice inputs.\n",
    "Developers must consider:\n",
    "\n",
    "* **Who collects the data?**\n",
    "* **How much data is stored?**\n",
    "* **What is it used for?**\n",
    "\n",
    "Transparency and responsible data management are essential to maintain user trust and protect privacy.\n",
    "\n",
    "---\n",
    "\n",
    "In short, building ethical NLP systems requires not just technical skill but **awareness, empathy, and accountability** — ensuring that language technologies serve *all* users fairly and responsibly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3caa706d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes classifies this as positive.\n",
      "\n",
      "According to our trained Naive Bayes classifier, the probability that your review was negative was 16.0% and the probability it was positive was 84.0%.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Add your review text containing slang or informal expressions\n",
    "# Normal expression\n",
    "review = \"It was fun!\"\n",
    "\n",
    "# Step 3: Convert your review into numerical token counts using the pre-trained vectorizer\n",
    "review_counts = counter.transform([review])\n",
    "\n",
    "# Step 4: Initialize the Naive Bayes classifier and define labels for training\n",
    "# 0 = negative review, 1 = positive review\n",
    "classifier = MultinomialNB()\n",
    "training_labels = [0] * 1000 + [1] * 1000\n",
    "\n",
    "# Step 5: Fit the classifier on the existing training dataset\n",
    "classifier.fit(training_counts, training_labels)\n",
    "\n",
    "# Step 6: Predict probabilities for the review being negative or positive\n",
    "neg = (classifier.predict_proba(review_counts)[0][0] * 100).round()\n",
    "pos = (classifier.predict_proba(review_counts)[0][1] * 100).round()\n",
    "\n",
    "# Step 7: Print classification result based on probability thresholds\n",
    "if pos > 50:\n",
    "    print(\"Naive Bayes classifies this as positive.\")\n",
    "elif neg > 50:\n",
    "    print(\"Naive Bayes classifies this as negative.\")\n",
    "else:\n",
    "    print(\"Naive Bayes cannot determine if this is negative or positive.\")\n",
    "\n",
    "# Step 8: Print the computed probabilities for both sentiment categories\n",
    "print(\n",
    "    \"\\nAccording to our trained Naive Bayes classifier, \"\n",
    "    \"the probability that your review was negative was {0}% \"\n",
    "    \"and the probability it was positive was {1}%.\".format(neg, pos)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c17440e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes classifies this as negative.\n",
      "\n",
      "According to our trained Naive Bayes classifier, the probability that your review was negative was 66.0% and the probability it was positive was 34.0%.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Add your review text containing slang or informal expressions\n",
    "# Try replacing with slang like \"lit\", \"dope\", \"fire\", or \"vibes\" to see how the model reacts.\n",
    "review = \"It was lit!\"\n",
    "\n",
    "# Step 3: Convert your review into numerical token counts using the pre-trained vectorizer\n",
    "review_counts = counter.transform([review])\n",
    "\n",
    "# Step 4: Initialize the Naive Bayes classifier and define labels for training\n",
    "# 0 = negative review, 1 = positive review\n",
    "classifier = MultinomialNB()\n",
    "training_labels = [0] * 1000 + [1] * 1000\n",
    "\n",
    "# Step 5: Fit the classifier on the existing training dataset\n",
    "classifier.fit(training_counts, training_labels)\n",
    "\n",
    "# Step 6: Predict probabilities for the review being negative or positive\n",
    "neg = (classifier.predict_proba(review_counts)[0][0] * 100).round()\n",
    "pos = (classifier.predict_proba(review_counts)[0][1] * 100).round()\n",
    "\n",
    "# Step 7: Print classification result based on probability thresholds\n",
    "if pos > 50:\n",
    "    print(\"Naive Bayes classifies this as positive.\")\n",
    "elif neg > 50:\n",
    "    print(\"Naive Bayes classifies this as negative.\")\n",
    "else:\n",
    "    print(\"Naive Bayes cannot determine if this is negative or positive.\")\n",
    "\n",
    "# Step 8: Print the computed probabilities for both sentiment categories\n",
    "print(\n",
    "    \"\\nAccording to our trained Naive Bayes classifier, \"\n",
    "    \"the probability that your review was negative was {0}% \"\n",
    "    \"and the probability it was positive was {1}%.\".format(neg, pos)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a599e793",
   "metadata": {},
   "source": [
    "# NLP Review — Final Remarks\n",
    "\n",
    "You’ve covered a lot of ground in understanding **Natural Language Processing (NLP)** — the field that blends **computer science**, **linguistics**, and **artificial intelligence** to help computers understand and generate human language.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "* **NLP Foundations**\n",
    "  NLP enables computers to process, interpret, and respond to human language in meaningful ways.\n",
    "\n",
    "* **NLTK Library**\n",
    "  Python’s **Natural Language Toolkit (NLTK)** provides a rich set of tools for tokenization, stemming, lemmatization, parsing, and more.\n",
    "\n",
    "* **Text Preprocessing**\n",
    "  Cleaning and preparing raw text — removing noise, normalizing case, and tokenizing — is a crucial first step for all NLP tasks.\n",
    "\n",
    "* **Parsing**\n",
    "  Parsing analyzes grammatical structure and syntax to understand relationships between words and phrases.\n",
    "\n",
    "* **Language Models**\n",
    "  Models like **Bag-of-Words**, **n-grams**, and **neural networks (LSTMs)** predict or generate text based on statistical or learned patterns.\n",
    "\n",
    "* **Topic Modeling**\n",
    "  Uncovers hidden themes or subjects in large text corpora using algorithms such as LDA (Latent Dirichlet Allocation).\n",
    "\n",
    "* **Text Similarity**\n",
    "  Measures how closely related two pieces of text are — essential for search, clustering, and recommendation systems.\n",
    "\n",
    "* **Language Prediction**\n",
    "  Powers applications like autocomplete, autosuggest, and smart replies by predicting likely next words or phrases.\n",
    "\n",
    "* **Ethical & Social Considerations**\n",
    "  NLP systems must address issues of **bias**, **fairness**, and **privacy**, ensuring inclusivity and responsible use of language data.\n",
    "\n",
    "---\n",
    "\n",
    "By mastering these concepts, you now have a strong foundation in how NLP helps machines *understand, analyze, and generate* human language — responsibly and intelligently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df82785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
